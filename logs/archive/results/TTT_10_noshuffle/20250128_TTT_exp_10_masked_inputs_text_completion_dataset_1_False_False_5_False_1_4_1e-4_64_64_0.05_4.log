
=== TTT: Loaded 27 tasks from BBH with k=10 ===
Finetuning steps: 1 (0 => skip), majority_vote=False, leave_one_out=False

=== PHASE 2: Multi-sample LoRA Finetuning ===
Running command: tune run lora_finetune_single_device --config /tmp/boolean_expressions_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/boolean_expressions_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/boolean_expressions_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114335.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 66.13 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.40s/it]1|1|Loss: 0.26688477396965027: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.40s/it]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/boolean_expressions_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/boolean_expressions_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/boolean_expressions_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/boolean_expressions_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 1.36 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.26688477396965027: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.76s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.09it/s][A
2|2|Loss: 0.0537022240459919: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.09it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/boolean_expressions_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/boolean_expressions_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/boolean_expressions_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/boolean_expressions_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 1.13 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.0537022240459919: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.33s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.20it/s]3|3|Loss: 0.006427307613193989: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.20it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/boolean_expressions_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/boolean_expressions_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/boolean_expressions_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/boolean_expressions_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.93 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.006427307613193989: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.13s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.27it/s][A
4|4|Loss: 0.001027149730361998: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.27it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/boolean_expressions_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/boolean_expressions_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/boolean_expressions_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.42 seconds.
4|4|Loss: 0.001027149730361998: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.63it/s]
[PHASE 2] Finished finetuning for boolean_expressions in 203.59s
Running command: tune run lora_finetune_single_device --config /tmp/causal_judgement_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/causal_judgement_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/causal_judgement_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114538.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 185.07 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.29it/s]1|1|Loss: 0.45840033888816833: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.29it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/causal_judgement_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/causal_judgement_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/causal_judgement_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/causal_judgement_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.85 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.45840033888816833: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.63s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.84it/s][A
2|2|Loss: 0.20410659909248352: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.84it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/causal_judgement_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/causal_judgement_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/causal_judgement_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/causal_judgement_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.71 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.20410659909248352: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.25s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.85it/s]3|3|Loss: 0.061645932495594025: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.85it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/causal_judgement_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/causal_judgement_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/causal_judgement_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/causal_judgement_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.71 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.061645932495594025: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.25s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.83it/s][A
4|4|Loss: 0.018199434503912926: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.83it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/causal_judgement_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/causal_judgement_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/causal_judgement_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.31 seconds.
4|4|Loss: 0.018199434503912926: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.16it/s]
[PHASE 2] Finished finetuning for causal_judgement in 16.28s
Running command: tune run lora_finetune_single_device --config /tmp/date_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/date_understanding_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/date_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114554.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 151.15 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.79it/s]1|1|Loss: 0.6796689033508301: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.79it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/date_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/date_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/date_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/date_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.78 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.6796689033508301: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.34s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.71it/s][A
2|2|Loss: 0.3004654347896576: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.71it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/date_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/date_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/date_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/date_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.69 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.3004654347896576: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.05it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.71it/s]3|3|Loss: 0.10233335942029953: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.71it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/date_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/date_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/date_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/date_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.72 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.10233335942029953: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.01it/s]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.70it/s][A
4|4|Loss: 0.027243763208389282: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.70it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/date_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/date_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/date_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.50 seconds.
4|4|Loss: 0.027243763208389282: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.29it/s]
[PHASE 2] Finished finetuning for date_understanding in 14.65s
Running command: tune run lora_finetune_single_device --config /tmp/disambiguation_qa_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/disambiguation_qa_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/disambiguation_qa_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114568.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 129.37 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.78it/s]1|1|Loss: 0.49748969078063965: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.78it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/disambiguation_qa_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/disambiguation_qa_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/disambiguation_qa_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/disambiguation_qa_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.77 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.49748969078063965: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.34s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.76it/s][A
2|2|Loss: 0.16926176846027374: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.76it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/disambiguation_qa_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/disambiguation_qa_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/disambiguation_qa_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/disambiguation_qa_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.73 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.16926176846027374: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.06it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.73it/s]3|3|Loss: 0.12205027043819427: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.73it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/disambiguation_qa_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/disambiguation_qa_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/disambiguation_qa_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/disambiguation_qa_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.86 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.12205027043819427: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.08s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.74it/s][A
4|4|Loss: 0.07459133863449097: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.74it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/disambiguation_qa_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/disambiguation_qa_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/disambiguation_qa_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.38 seconds.
4|4|Loss: 0.07459133863449097: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.70it/s]
[PHASE 2] Finished finetuning for disambiguation_qa in 14.92s
Running command: tune run lora_finetune_single_device --config /tmp/dyck_languages_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/dyck_languages_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/dyck_languages_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114584.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 197.32 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.77it/s]1|1|Loss: 1.8703510761260986: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.77it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/dyck_languages_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/dyck_languages_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/dyck_languages_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/dyck_languages_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.87 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 1.8703510761260986: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.44s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.73it/s][A
2|2|Loss: 0.8362166285514832: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.73it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/dyck_languages_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/dyck_languages_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/dyck_languages_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/dyck_languages_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.78 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.8362166285514832: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.01it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.96it/s]3|3|Loss: 0.43469345569610596: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.96it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/dyck_languages_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/dyck_languages_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/dyck_languages_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/dyck_languages_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.99 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.43469345569610596: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.20s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.10it/s][A
4|4|Loss: 0.17128247022628784: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.10it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/dyck_languages_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/dyck_languages_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/dyck_languages_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.44 seconds.
4|4|Loss: 0.17128247022628784: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.58it/s]
[PHASE 2] Finished finetuning for dyck_languages in 15.48s
Running command: tune run lora_finetune_single_device --config /tmp/formal_fallacies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/formal_fallacies_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/formal_fallacies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114599.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 185.06 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.81it/s]1|1|Loss: 0.4414350092411041: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.81it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/formal_fallacies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/formal_fallacies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/formal_fallacies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/formal_fallacies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.77 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.4414350092411041: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.32s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.32it/s][A
2|2|Loss: 0.12809854745864868: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.32it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/formal_fallacies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/formal_fallacies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/formal_fallacies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/formal_fallacies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.67 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.12809854745864868: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.03it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.33it/s]3|3|Loss: 0.017989089712500572: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.33it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/formal_fallacies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/formal_fallacies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/formal_fallacies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/formal_fallacies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.89 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.017989089712500572: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.19s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.34it/s][A
4|4|Loss: 0.005643731448799372: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.34it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/formal_fallacies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/formal_fallacies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/formal_fallacies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.39 seconds.
4|4|Loss: 0.005643731448799372: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.45it/s]
[PHASE 2] Finished finetuning for formal_fallacies in 14.82s
Running command: tune run lora_finetune_single_device --config /tmp/geometric_shapes_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/geometric_shapes_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/geometric_shapes_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114614.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 181.49 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.86it/s]1|1|Loss: 0.4888803958892822: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.86it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/geometric_shapes_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/geometric_shapes_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/geometric_shapes_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/geometric_shapes_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.79 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.4888803958892822: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.33s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.36it/s][A
2|2|Loss: 0.18717408180236816: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.36it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/geometric_shapes_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/geometric_shapes_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/geometric_shapes_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/geometric_shapes_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.68 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.18717408180236816: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.02it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.36it/s]3|3|Loss: 0.05324196070432663: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.36it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/geometric_shapes_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/geometric_shapes_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/geometric_shapes_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/geometric_shapes_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.85 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.05324196070432663: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.15s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.35it/s][A
4|4|Loss: 0.0179549939930439: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.35it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/geometric_shapes_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/geometric_shapes_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/geometric_shapes_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.43 seconds.
4|4|Loss: 0.0179549939930439: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.38it/s]
[PHASE 2] Finished finetuning for geometric_shapes in 14.60s
Running command: tune run lora_finetune_single_device --config /tmp/hyperbaton_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/hyperbaton_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/hyperbaton_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114628.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 145.99 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.29it/s]1|1|Loss: 0.3648144006729126: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.29it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/hyperbaton_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/hyperbaton_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/hyperbaton_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/hyperbaton_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 1.01 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.3648144006729126: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.45s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.99it/s][A
2|2|Loss: 0.1979156732559204: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.99it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/hyperbaton_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/hyperbaton_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/hyperbaton_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/hyperbaton_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.92 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.1979156732559204: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.12s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.82it/s]3|3|Loss: 0.12917721271514893: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.82it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/hyperbaton_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/hyperbaton_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/hyperbaton_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/hyperbaton_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 1.19 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.12917721271514893: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.40s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.99it/s][A
4|4|Loss: 0.08507846295833588: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.99it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/hyperbaton_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/hyperbaton_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/hyperbaton_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.36 seconds.
4|4|Loss: 0.08507846295833588: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.78it/s]
[PHASE 2] Finished finetuning for hyperbaton in 14.87s
Running command: tune run lora_finetune_single_device --config /tmp/logical_deduction_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/logical_deduction_five_objects_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/logical_deduction_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114643.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 167.31 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.56it/s]1|1|Loss: 0.5693756937980652: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.56it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/logical_deduction_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/logical_deduction_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.99 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.5693756937980652: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.63s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.87it/s][A
2|2|Loss: 0.3105810880661011: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.87it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/logical_deduction_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/logical_deduction_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.90 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.3105810880661011: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.25s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.88it/s]3|3|Loss: 0.1057073101401329: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.88it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/logical_deduction_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/logical_deduction_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.75 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.1057073101401329: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.10s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.86it/s][A
4|4|Loss: 0.012700827792286873: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.86it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/logical_deduction_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.31 seconds.
4|4|Loss: 0.012700827792286873: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.51it/s]
[PHASE 2] Finished finetuning for logical_deduction_five_objects in 15.23s
Running command: tune run lora_finetune_single_device --config /tmp/logical_deduction_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/logical_deduction_seven_objects_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/logical_deduction_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114658.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 197.72 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.41it/s]1|1|Loss: 0.63224196434021: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.41it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/logical_deduction_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/logical_deduction_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.82 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.63224196434021: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.53s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.96it/s][A
2|2|Loss: 0.3275468349456787: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.96it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/logical_deduction_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/logical_deduction_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.67 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.3275468349456787: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.18s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.95it/s]3|3|Loss: 0.13663874566555023: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.95it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/logical_deduction_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/logical_deduction_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.67 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.13663874566555023: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.18s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.95it/s][A
4|4|Loss: 0.058637283742427826: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.95it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/logical_deduction_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.31 seconds.
4|4|Loss: 0.058637283742427826: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.22it/s]
[PHASE 2] Finished finetuning for logical_deduction_seven_objects in 15.34s
Running command: tune run lora_finetune_single_device --config /tmp/logical_deduction_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/logical_deduction_three_objects_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/logical_deduction_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114674.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 182.12 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.96it/s]1|1|Loss: 0.2636965215206146: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.96it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/logical_deduction_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/logical_deduction_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 1.04 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.2636965215206146: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.55s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.70it/s][A
2|2|Loss: 0.0578027181327343: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.70it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/logical_deduction_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/logical_deduction_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.68 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.0578027181327343: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.05it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.69it/s]3|3|Loss: 0.0044501833617687225: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.69it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/logical_deduction_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/logical_deduction_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.64 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.0044501833617687225: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.09it/s]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.69it/s][A
4|4|Loss: 0.00023738663003314286: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.69it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/logical_deduction_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/logical_deduction_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.31 seconds.
4|4|Loss: 0.00023738663003314286: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.73it/s]
[PHASE 2] Finished finetuning for logical_deduction_three_objects in 14.30s
Running command: tune run lora_finetune_single_device --config /tmp/movie_recommendation_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/movie_recommendation_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/movie_recommendation_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114688.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 184.89 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.11it/s]1|1|Loss: 0.3806939721107483: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.11it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/movie_recommendation_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/movie_recommendation_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/movie_recommendation_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/movie_recommendation_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.83 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.3806939721107483: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.30s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.85it/s][A
2|2|Loss: 0.034004922956228256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.85it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/movie_recommendation_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/movie_recommendation_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/movie_recommendation_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/movie_recommendation_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.73 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.034004922956228256: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.06it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.67it/s]3|3|Loss: 0.0008244355558417737: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.67it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/movie_recommendation_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/movie_recommendation_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/movie_recommendation_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/movie_recommendation_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.75 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.0008244355558417737: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.03it/s]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.96it/s][A
4|4|Loss: 0.00016388836957048625: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.96it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/movie_recommendation_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/movie_recommendation_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/movie_recommendation_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.32 seconds.
4|4|Loss: 0.00016388836957048625: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.92it/s]
[PHASE 2] Finished finetuning for movie_recommendation in 14.01s
Running command: tune run lora_finetune_single_device --config /tmp/multistep_arithmetic_two_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/multistep_arithmetic_two_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/multistep_arithmetic_two_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114702.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 189.80 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.09it/s]1|1|Loss: 1.9595474004745483: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.09it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/multistep_arithmetic_two_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/multistep_arithmetic_two_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/multistep_arithmetic_two_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/multistep_arithmetic_two_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.79 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 1.9595474004745483: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.27s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.05it/s][A
2|2|Loss: 1.5997835397720337: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.05it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/multistep_arithmetic_two_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/multistep_arithmetic_two_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/multistep_arithmetic_two_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/multistep_arithmetic_two_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.97 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 1.5997835397720337: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.17s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.01it/s]3|3|Loss: 1.25016188621521: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.01it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/multistep_arithmetic_two_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/multistep_arithmetic_two_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/multistep_arithmetic_two_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/multistep_arithmetic_two_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.76 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 1.25016188621521: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.04it/s]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.11it/s][A
4|4|Loss: 0.995476484298706: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.11it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/multistep_arithmetic_two_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/multistep_arithmetic_two_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/multistep_arithmetic_two_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.34 seconds.
4|4|Loss: 0.995476484298706: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.88it/s]
[PHASE 2] Finished finetuning for multistep_arithmetic_two in 14.29s
Running command: tune run lora_finetune_single_device --config /tmp/navigate_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/navigate_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/navigate_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114716.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 179.76 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.13it/s]1|1|Loss: 0.5486182570457458: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.13it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/navigate_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/navigate_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/navigate_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/navigate_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.80 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.5486182570457458: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.27s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.60it/s][A
2|2|Loss: 0.3125053346157074: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.60it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/navigate_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/navigate_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/navigate_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/navigate_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.76 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.3125053346157074: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.02it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.93it/s]3|3|Loss: 0.10780882090330124: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.93it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/navigate_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/navigate_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/navigate_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/navigate_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.78 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.10780882090330124: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.01it/s]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.00it/s][A
4|4|Loss: 0.06777156889438629: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.00it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/navigate_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/navigate_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/navigate_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.32 seconds.
4|4|Loss: 0.06777156889438629: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.92it/s]
[PHASE 2] Finished finetuning for navigate in 14.21s
Running command: tune run lora_finetune_single_device --config /tmp/object_counting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/object_counting_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/object_counting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114730.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 182.24 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.10it/s]1|1|Loss: 0.6624969840049744: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.10it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/object_counting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/object_counting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/object_counting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/object_counting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.77 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.6624969840049744: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.25s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.78it/s][A
2|2|Loss: 0.46284496784210205: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.78it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/object_counting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/object_counting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/object_counting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/object_counting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.82 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.46284496784210205: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.03s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.86it/s]3|3|Loss: 0.19830456376075745: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.86it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/object_counting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/object_counting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/object_counting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/object_counting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.91 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.19830456376075745: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.12s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.96it/s][A
4|4|Loss: 0.11562714725732803: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.96it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/object_counting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/object_counting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/object_counting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.31 seconds.
4|4|Loss: 0.11562714725732803: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.96it/s]
[PHASE 2] Finished finetuning for object_counting in 14.47s
Running command: tune run lora_finetune_single_device --config /tmp/penguins_in_a_table_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/penguins_in_a_table_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/penguins_in_a_table_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114745.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 179.35 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.55it/s]1|1|Loss: 0.4597802758216858: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.55it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/penguins_in_a_table_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/penguins_in_a_table_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/penguins_in_a_table_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/penguins_in_a_table_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.76 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.4597802758216858: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.41s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.51it/s][A
2|2|Loss: 0.18913249671459198: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.51it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/penguins_in_a_table_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/penguins_in_a_table_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/penguins_in_a_table_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/penguins_in_a_table_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.65 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.18913249671459198: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.05s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.51it/s]3|3|Loss: 0.07583407312631607: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.51it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/penguins_in_a_table_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/penguins_in_a_table_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/penguins_in_a_table_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/penguins_in_a_table_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.67 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.07583407312631607: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.07s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.50it/s][A
4|4|Loss: 0.026650993153452873: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.50it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/penguins_in_a_table_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/penguins_in_a_table_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/penguins_in_a_table_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.31 seconds.
4|4|Loss: 0.026650993153452873: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.41it/s]
[PHASE 2] Finished finetuning for penguins_in_a_table in 14.93s
Running command: tune run lora_finetune_single_device --config /tmp/reasoning_about_colored_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/reasoning_about_colored_objects_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/reasoning_about_colored_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114759.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 165.82 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.75it/s]1|1|Loss: 0.5296794772148132: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.75it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/reasoning_about_colored_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/reasoning_about_colored_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/reasoning_about_colored_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/reasoning_about_colored_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.79 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.5296794772148132: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.37s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.12it/s][A
2|2|Loss: 0.1874082386493683: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.12it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/reasoning_about_colored_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/reasoning_about_colored_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/reasoning_about_colored_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/reasoning_about_colored_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.71 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.1874082386493683: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.03s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.13it/s]3|3|Loss: 0.040785208344459534: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.13it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/reasoning_about_colored_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/reasoning_about_colored_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/reasoning_about_colored_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/reasoning_about_colored_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.87 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.040785208344459534: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.19s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.12it/s][A
4|4|Loss: 0.018550075590610504: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.12it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/reasoning_about_colored_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/reasoning_about_colored_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/reasoning_about_colored_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.33 seconds.
4|4|Loss: 0.018550075590610504: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.54it/s]
[PHASE 2] Finished finetuning for reasoning_about_colored_objects in 14.65s
Running command: tune run lora_finetune_single_device --config /tmp/ruin_names_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/ruin_names_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/ruin_names_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114774.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 180.88 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.07it/s]1|1|Loss: 0.38372063636779785: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.07it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/ruin_names_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/ruin_names_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/ruin_names_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/ruin_names_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.87 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.38372063636779785: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.36s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.79it/s][A
2|2|Loss: 0.03934745490550995: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.79it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/ruin_names_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/ruin_names_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/ruin_names_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/ruin_names_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.85 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.03934745490550995: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.06s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.87it/s]3|3|Loss: 0.0010138057405129075: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.87it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/ruin_names_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/ruin_names_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/ruin_names_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/ruin_names_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.74 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.0010138057405129075: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.06it/s]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.97it/s][A
4|4|Loss: 0.00016716784739401191: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.97it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/ruin_names_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/ruin_names_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/ruin_names_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.30 seconds.
4|4|Loss: 0.00016716784739401191: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.98it/s]
[PHASE 2] Finished finetuning for ruin_names in 14.65s
Running command: tune run lora_finetune_single_device --config /tmp/salient_translation_error_detection_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/salient_translation_error_detection_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/salient_translation_error_detection_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114789.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 187.77 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.10it/s]1|1|Loss: 0.7874507308006287: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.10it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/salient_translation_error_detection_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/salient_translation_error_detection_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/salient_translation_error_detection_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/salient_translation_error_detection_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.77 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.7874507308006287: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.68s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.51it/s][A
2|2|Loss: 0.4507266581058502: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.51it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/salient_translation_error_detection_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/salient_translation_error_detection_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/salient_translation_error_detection_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/salient_translation_error_detection_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.65 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.4507266581058502: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.32s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.51it/s]3|3|Loss: 0.3048810362815857: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.51it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/salient_translation_error_detection_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/salient_translation_error_detection_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/salient_translation_error_detection_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/salient_translation_error_detection_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.67 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.3048810362815857: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.33s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.51it/s][A
4|4|Loss: 0.2210247963666916: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.51it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/salient_translation_error_detection_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/salient_translation_error_detection_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/salient_translation_error_detection_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.31 seconds.
4|4|Loss: 0.2210247963666916: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.03it/s]
[PHASE 2] Finished finetuning for salient_translation_error_detection in 15.59s
Running command: tune run lora_finetune_single_device --config /tmp/snarks_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/snarks_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/snarks_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114805.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 185.51 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.09it/s]1|1|Loss: 0.2976296842098236: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.09it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/snarks_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/snarks_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/snarks_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/snarks_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.92 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.2976296842098236: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.40s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.74it/s][A
2|2|Loss: 0.0634971559047699: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.74it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/snarks_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/snarks_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/snarks_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/snarks_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.88 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.0634971559047699: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.09s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.88it/s]3|3|Loss: 0.005245119333267212: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.88it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/snarks_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/snarks_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/snarks_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/snarks_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.78 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.005245119333267212: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.02it/s]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.86it/s][A
4|4|Loss: 0.0012841966236010194: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.86it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/snarks_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/snarks_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/snarks_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.33 seconds.
4|4|Loss: 0.0012841966236010194: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.86it/s]
[PHASE 2] Finished finetuning for snarks in 14.54s
Running command: tune run lora_finetune_single_device --config /tmp/sports_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/sports_understanding_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/sports_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114819.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 180.14 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.03it/s]1|1|Loss: 0.08888550102710724: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.03it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/sports_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/sports_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/sports_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/sports_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.88 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.08888550102710724: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.37s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.57it/s][A
2|2|Loss: 0.010720768943428993: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.57it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/sports_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/sports_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/sports_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/sports_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.78 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.010720768943428993: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.00s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.95it/s]3|3|Loss: 0.00017677928553894162: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.95it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/sports_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/sports_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/sports_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/sports_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.70 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.00017677928553894162: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.10it/s]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.97it/s][A
4|4|Loss: 2.2795689801569097e-05: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.97it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/sports_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/sports_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/sports_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.36 seconds.
4|4|Loss: 2.2795689801569097e-05: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.78it/s]
[PHASE 2] Finished finetuning for sports_understanding in 14.30s
Running command: tune run lora_finetune_single_device --config /tmp/temporal_sequences_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/temporal_sequences_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/temporal_sequences_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114833.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 99.12 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.46it/s]1|1|Loss: 0.7139313817024231: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.46it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/temporal_sequences_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/temporal_sequences_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/temporal_sequences_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/temporal_sequences_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.79 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.7139313817024231: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.47s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.47it/s][A
2|2|Loss: 0.10084356367588043: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.47it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/temporal_sequences_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/temporal_sequences_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/temporal_sequences_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/temporal_sequences_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.68 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.10084356367588043: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.08s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.46it/s]3|3|Loss: 0.005686120595782995: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.46it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/temporal_sequences_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/temporal_sequences_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/temporal_sequences_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/temporal_sequences_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.66 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.005686120595782995: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.06s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.46it/s][A
4|4|Loss: 0.0020190952345728874: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.46it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/temporal_sequences_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/temporal_sequences_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/temporal_sequences_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.31 seconds.
4|4|Loss: 0.0020190952345728874: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.39it/s]
[PHASE 2] Finished finetuning for temporal_sequences in 14.79s
Running command: tune run lora_finetune_single_device --config /tmp/tracking_shuffled_objects_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/tracking_shuffled_objects_five_objects_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/tracking_shuffled_objects_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114848.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 178.28 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.46it/s]1|1|Loss: 0.7107380628585815: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.46it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/tracking_shuffled_objects_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/tracking_shuffled_objects_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.84 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.7107380628585815: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.52s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.52it/s][A
2|2|Loss: 0.4699300229549408: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.52it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/tracking_shuffled_objects_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/tracking_shuffled_objects_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.72 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.4699300229549408: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.11s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.53it/s]3|3|Loss: 0.24483025074005127: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.53it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/tracking_shuffled_objects_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/tracking_shuffled_objects_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.65 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.24483025074005127: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.05s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.53it/s][A
4|4|Loss: 0.08833984285593033: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.53it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/tracking_shuffled_objects_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.32 seconds.
4|4|Loss: 0.08833984285593033: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.40it/s]
[PHASE 2] Finished finetuning for tracking_shuffled_objects_five_objects in 14.75s
Running command: tune run lora_finetune_single_device --config /tmp/tracking_shuffled_objects_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/tracking_shuffled_objects_seven_objects_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/tracking_shuffled_objects_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114863.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 185.97 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.30it/s]1|1|Loss: 0.8815474510192871: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.30it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/tracking_shuffled_objects_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/tracking_shuffled_objects_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.81 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.8815474510192871: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.58s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.87it/s][A
2|2|Loss: 0.6594646573066711: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.87it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/tracking_shuffled_objects_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/tracking_shuffled_objects_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.66 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.6594646573066711: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.20s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.87it/s]3|3|Loss: 0.5582603216171265: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.87it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/tracking_shuffled_objects_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/tracking_shuffled_objects_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.68 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.5582603216171265: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.22s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.87it/s][A
4|4|Loss: 0.4949677586555481: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.87it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/tracking_shuffled_objects_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.30 seconds.
4|4|Loss: 0.4949677586555481: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.19it/s]
[PHASE 2] Finished finetuning for tracking_shuffled_objects_seven_objects in 15.40s
Running command: tune run lora_finetune_single_device --config /tmp/tracking_shuffled_objects_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/tracking_shuffled_objects_three_objects_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/tracking_shuffled_objects_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114878.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 186.25 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.62it/s]1|1|Loss: 0.4677734076976776: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.62it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/tracking_shuffled_objects_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/tracking_shuffled_objects_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.80 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.4677734076976776: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.42s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.39it/s][A
2|2|Loss: 0.34950926899909973: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.39it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/tracking_shuffled_objects_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/tracking_shuffled_objects_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.93 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.34950926899909973: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.23s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.39it/s]3|3|Loss: 0.18794679641723633: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.39it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/tracking_shuffled_objects_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/tracking_shuffled_objects_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.87 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.18794679641723633: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.17s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.39it/s][A
4|4|Loss: 0.10190192610025406: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.39it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/tracking_shuffled_objects_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/tracking_shuffled_objects_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.47 seconds.
4|4|Loss: 0.10190192610025406: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.31it/s]
[PHASE 2] Finished finetuning for tracking_shuffled_objects_three_objects in 15.13s
Running command: tune run lora_finetune_single_device --config /tmp/web_of_lies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/web_of_lies_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/web_of_lies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114893.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 185.54 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.64it/s]1|1|Loss: 0.9131336212158203: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.64it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/web_of_lies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/web_of_lies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/web_of_lies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/web_of_lies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.92 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.9131336212158203: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.53s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.81it/s][A
2|2|Loss: 0.339006245136261: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.81it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/web_of_lies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/web_of_lies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/web_of_lies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/web_of_lies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.93 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.339006245136261: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.14s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.03it/s]3|3|Loss: 0.28321054577827454: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.03it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/web_of_lies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/web_of_lies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/web_of_lies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/web_of_lies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.88 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.28321054577827454: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.08s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.03it/s][A
4|4|Loss: 0.21468475461006165: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.03it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/web_of_lies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/web_of_lies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/web_of_lies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.54 seconds.
4|4|Loss: 0.21468475461006165: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.36it/s]
[PHASE 2] Finished finetuning for web_of_lies in 14.77s
Running command: tune run lora_finetune_single_device --config /tmp/word_sorting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/word_sorting_config.yaml
Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/word_sorting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/log_1738114908.txt
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.33 GiB
	GPU peak memory reserved: 15.35 GiB
	GPU peak memory active: 15.33 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00, 172.98 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.04it/s]1|1|Loss: 0.07202643901109695: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.04it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/word_sorting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_0.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/word_sorting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/word_sorting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/word_sorting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.85 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A1|1|Loss: 0.07202643901109695: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.34s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.14it/s][A
2|2|Loss: 0.042373135685920715: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.14it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/word_sorting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_1.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/word_sorting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/word_sorting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/word_sorting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.69 seconds.
  0%|          | 0/1 [00:00<?, ?it/s]2|2|Loss: 0.042373135685920715: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.13it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.07it/s]3|3|Loss: 0.01494789682328701: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.07it/s]Starting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/word_sorting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_2.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/word_sorting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/word_sorting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Recipe checkpoint of size 0.63 GB saved to /tmp/word_sorting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/recipe_state.pt
Checkpoint saved in 0.68 seconds.

  0%|          | 0/1 [00:00<?, ?it/s][A3|3|Loss: 0.01494789682328701: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.14it/s]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.18it/s][A
4|4|Loss: 0.010993233881890774: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.18it/s][AStarting checkpoint save...
Adapter checkpoint of size 0.31 GB saved to /tmp/word_sorting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_3.pt
Adapter checkpoint of size 0.31 GB saved to /tmp/word_sorting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_model.bin
Adapter checkpoint of size 0.00 GB saved to /tmp/word_sorting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter/adapter_config.json
Saving final epoch checkpoint.
Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.
Checkpoint saved in 0.30 seconds.
4|4|Loss: 0.010993233881890774: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.03it/s]
[PHASE 2] Finished finetuning for word_sorting in 14.13s

=== PHASE 3: Evaluation with vLLM + LoRA ===
INFO 01-28 20:42:04 config.py:478] This model supports multiple tasks: {'reward', 'embed', 'generate', 'classify', 'score'}. Defaulting to 'generate'.
INFO 01-28 20:42:04 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='/data/cl/u/adamz/Models/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='/data/cl/u/adamz/Models/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/data/cl/u/adamz/Models/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 01-28 20:42:05 selector.py:120] Using Flash Attention backend.
INFO 01-28 20:42:06 model_runner.py:1092] Starting to load model /data/cl/u/adamz/Models/Llama-3.1-8B-Instruct...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.34it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.22it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.27it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.72it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.52it/s]

INFO 01-28 20:42:09 model_runner.py:1097] Loading model weights took 14.9927 GB
INFO 01-28 20:42:09 punica_selector.py:11] Using PunicaWrapperGPU.
INFO 01-28 20:42:10 worker.py:241] Memory profiling takes 1.43 seconds
INFO 01-28 20:42:10 worker.py:241] the current vLLM instance can use total_gpu_memory (79.21GiB) x gpu_memory_utilization (0.90) = 71.29GiB
INFO 01-28 20:42:10 worker.py:241] model weights take 14.99GiB; non_torch_memory takes 0.52GiB; PyTorch activation peak memory takes 1.23GiB; the rest of the memory reserved for KV Cache is 54.55GiB.
INFO 01-28 20:42:10 gpu_executor.py:76] # GPU blocks: 27928, # CPU blocks: 2048
INFO 01-28 20:42:10 gpu_executor.py:80] Maximum concurrency for 4096 tokens per request: 109.09x
INFO 01-28 20:42:13 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-28 20:42:13 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-28 20:42:29 model_runner.py:1527] Graph capturing finished in 16 secs, took 0.51 GiB
INFO 01-28 20:42:29 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 19.93 seconds
/data/cl/u/adamz/Fewshot-TTT/src/methods/ttt.py:488: DeprecationWarning: The 'lora_local_path' attribute is deprecated and will be removed in a future version. Please use 'lora_path' instead.
  lora_request = LoRARequest(
WARNING 01-28 20:42:29 tokenizer.py:191] No tokenizer found in /tmp/boolean_expressions_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/boolean_expressions_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/boolean_expressions_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]/data/cl/u/adamz/miniconda3/envs/tttenv/lib/python3.12/site-packages/vllm/lora/models.py:270: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tensors = torch.load(lora_bin_file_path, map_location=device)
Processed prompts:   0%|          | 1/240 [00:01<05:16,  1.32s/it, est. speed input: 146.46 toks/s, output: 0.75 toks/s]Processed prompts:   9%|â–‰         | 22/240 [00:01<00:11, 19.12it/s, est. speed input: 2785.27 toks/s, output: 14.36 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 43/240 [00:01<00:05, 36.12it/s, est. speed input: 4794.46 toks/s, output: 24.71 toks/s]Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 64/240 [00:01<00:03, 50.16it/s, est. speed input: 6327.76 toks/s, output: 32.62 toks/s]Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 85/240 [00:02<00:02, 61.92it/s, est. speed input: 7569.44 toks/s, output: 39.02 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 106/240 [00:02<00:01, 71.88it/s, est. speed input: 8615.31 toks/s, output: 44.41 toks/s]Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 127/240 [00:02<00:01, 79.61it/s, est. speed input: 9492.55 toks/s, output: 48.93 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 148/240 [00:02<00:01, 85.04it/s, est. speed input: 10226.22 toks/s, output: 52.71 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 169/240 [00:03<00:00, 88.33it/s, est. speed input: 10835.64 toks/s, output: 55.85 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 190/240 [00:03<00:00, 91.09it/s, est. speed input: 11374.97 toks/s, output: 58.63 toks/s]Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 232/240 [00:03<00:00, 141.18it/s, est. speed input: 13416.48 toks/s, output: 69.16 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:03<00:00, 71.54it/s, est. speed input: 13878.29 toks/s, output: 71.54 toks/s] 
[PHASE 3] Task=boolean_expressions, accuracy=84.17%, ft_time=203.59s, eval_time=3.47s
WARNING 01-28 20:42:32 tokenizer.py:191] No tokenizer found in /tmp/causal_judgement_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/causal_judgement_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/causal_judgement_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/177 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/177 [00:00<01:23,  2.11it/s, est. speed input: 5106.92 toks/s, output: 2.11 toks/s]Processed prompts:   1%|          | 2/177 [00:00<00:48,  3.64it/s, est. speed input: 8524.57 toks/s, output: 3.29 toks/s]Processed prompts:   2%|â–         | 3/177 [00:00<00:36,  4.71it/s, est. speed input: 10114.58 toks/s, output: 4.02 toks/s]Processed prompts:   2%|â–         | 4/177 [00:00<00:31,  5.51it/s, est. speed input: 11384.50 toks/s, output: 4.54 toks/s]Processed prompts:   3%|â–Ž         | 5/177 [00:01<00:28,  6.01it/s, est. speed input: 12159.70 toks/s, output: 4.90 toks/s]Processed prompts:   3%|â–Ž         | 6/177 [00:01<00:27,  6.28it/s, est. speed input: 12807.87 toks/s, output: 5.14 toks/s]Processed prompts:   4%|â–         | 7/177 [00:01<00:26,  6.49it/s, est. speed input: 13288.46 toks/s, output: 5.35 toks/s]Processed prompts:   5%|â–         | 8/177 [00:01<00:25,  6.71it/s, est. speed input: 13740.23 toks/s, output: 5.52 toks/s]Processed prompts:   5%|â–Œ         | 9/177 [00:01<00:24,  6.83it/s, est. speed input: 14052.86 toks/s, output: 5.67 toks/s]Processed prompts:   6%|â–Œ         | 10/177 [00:01<00:23,  6.96it/s, est. speed input: 14346.34 toks/s, output: 5.79 toks/s]Processed prompts:   6%|â–Œ         | 11/177 [00:01<00:23,  7.03it/s, est. speed input: 14572.76 toks/s, output: 5.90 toks/s]Processed prompts:   7%|â–‹         | 12/177 [00:02<00:23,  7.11it/s, est. speed input: 14805.81 toks/s, output: 5.99 toks/s]Processed prompts:   7%|â–‹         | 13/177 [00:02<00:23,  7.06it/s, est. speed input: 14952.93 toks/s, output: 6.06 toks/s]Processed prompts:   8%|â–Š         | 14/177 [00:02<00:23,  7.08it/s, est. speed input: 15121.47 toks/s, output: 6.12 toks/s]Processed prompts:   8%|â–Š         | 15/177 [00:02<00:22,  7.12it/s, est. speed input: 15237.89 toks/s, output: 6.18 toks/s]Processed prompts:   9%|â–‰         | 16/177 [00:02<00:22,  7.15it/s, est. speed input: 15378.62 toks/s, output: 6.24 toks/s]Processed prompts:  10%|â–‰         | 17/177 [00:02<00:22,  7.23it/s, est. speed input: 15491.08 toks/s, output: 6.30 toks/s]Processed prompts:  10%|â–ˆ         | 18/177 [00:02<00:21,  7.28it/s, est. speed input: 15604.72 toks/s, output: 6.35 toks/s]Processed prompts:  11%|â–ˆ         | 19/177 [00:02<00:21,  7.28it/s, est. speed input: 15668.96 toks/s, output: 6.40 toks/s]Processed prompts:  11%|â–ˆâ–        | 20/177 [00:03<00:21,  7.24it/s, est. speed input: 15738.04 toks/s, output: 6.43 toks/s]Processed prompts:  12%|â–ˆâ–        | 21/177 [00:03<00:21,  7.16it/s, est. speed input: 15789.48 toks/s, output: 6.45 toks/s]Processed prompts:  12%|â–ˆâ–        | 22/177 [00:03<00:21,  7.15it/s, est. speed input: 15873.86 toks/s, output: 6.48 toks/s]Processed prompts:  13%|â–ˆâ–Ž        | 23/177 [00:03<00:21,  7.17it/s, est. speed input: 15929.04 toks/s, output: 6.51 toks/s]Processed prompts:  14%|â–ˆâ–Ž        | 24/177 [00:03<00:21,  7.14it/s, est. speed input: 15964.96 toks/s, output: 6.53 toks/s]Processed prompts:  14%|â–ˆâ–        | 25/177 [00:03<00:21,  7.17it/s, est. speed input: 16034.92 toks/s, output: 6.56 toks/s]Processed prompts:  15%|â–ˆâ–        | 26/177 [00:03<00:21,  7.18it/s, est. speed input: 16062.87 toks/s, output: 6.58 toks/s]Processed prompts:  15%|â–ˆâ–Œ        | 27/177 [00:04<00:20,  7.18it/s, est. speed input: 16108.83 toks/s, output: 6.60 toks/s]Processed prompts:  16%|â–ˆâ–Œ        | 28/177 [00:04<00:20,  7.13it/s, est. speed input: 16133.89 toks/s, output: 6.62 toks/s]Processed prompts:  16%|â–ˆâ–‹        | 29/177 [00:04<00:20,  7.14it/s, est. speed input: 16183.76 toks/s, output: 6.63 toks/s]Processed prompts:  17%|â–ˆâ–‹        | 30/177 [00:04<00:20,  7.12it/s, est. speed input: 16211.88 toks/s, output: 6.65 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 31/177 [00:04<00:20,  6.96it/s, est. speed input: 16208.19 toks/s, output: 6.65 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 32/177 [00:04<00:21,  6.89it/s, est. speed input: 16263.44 toks/s, output: 6.65 toks/s]Processed prompts:  19%|â–ˆâ–Š        | 33/177 [00:04<00:20,  6.98it/s, est. speed input: 16328.72 toks/s, output: 6.66 toks/s]Processed prompts:  19%|â–ˆâ–‰        | 34/177 [00:05<00:20,  7.02it/s, est. speed input: 16350.73 toks/s, output: 6.68 toks/s]Processed prompts:  20%|â–ˆâ–‰        | 35/177 [00:05<00:20,  6.78it/s, est. speed input: 16319.72 toks/s, output: 6.66 toks/s]Processed prompts:  20%|â–ˆâ–ˆ        | 36/177 [00:05<00:20,  6.90it/s, est. speed input: 16408.63 toks/s, output: 6.68 toks/s]Processed prompts:  21%|â–ˆâ–ˆ        | 37/177 [00:05<00:20,  6.97it/s, est. speed input: 16430.09 toks/s, output: 6.69 toks/s]Processed prompts:  21%|â–ˆâ–ˆâ–       | 38/177 [00:05<00:19,  7.01it/s, est. speed input: 16448.80 toks/s, output: 6.70 toks/s]Processed prompts:  22%|â–ˆâ–ˆâ–       | 39/177 [00:05<00:19,  7.08it/s, est. speed input: 16482.83 toks/s, output: 6.71 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 40/177 [00:05<00:19,  6.93it/s, est. speed input: 16463.01 toks/s, output: 6.71 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 41/177 [00:06<00:19,  6.96it/s, est. speed input: 16513.38 toks/s, output: 6.72 toks/s]Processed prompts:  24%|â–ˆâ–ˆâ–Ž       | 42/177 [00:06<00:19,  7.03it/s, est. speed input: 16546.39 toks/s, output: 6.73 toks/s]Processed prompts:  24%|â–ˆâ–ˆâ–       | 43/177 [00:06<00:18,  7.07it/s, est. speed input: 16559.62 toks/s, output: 6.74 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–       | 44/177 [00:06<00:19,  6.96it/s, est. speed input: 16560.14 toks/s, output: 6.74 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 45/177 [00:06<00:18,  7.00it/s, est. speed input: 16605.72 toks/s, output: 6.75 toks/s]Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 46/177 [00:06<00:18,  7.07it/s, est. speed input: 16630.27 toks/s, output: 6.76 toks/s]Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 47/177 [00:06<00:18,  7.09it/s, est. speed input: 16638.08 toks/s, output: 6.76 toks/s]Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 48/177 [00:07<00:18,  7.08it/s, est. speed input: 16645.39 toks/s, output: 6.77 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 49/177 [00:07<00:18,  7.09it/s, est. speed input: 16659.80 toks/s, output: 6.78 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 50/177 [00:07<00:17,  7.09it/s, est. speed input: 16669.36 toks/s, output: 6.78 toks/s]Processed prompts:  29%|â–ˆâ–ˆâ–‰       | 51/177 [00:07<00:17,  7.11it/s, est. speed input: 16686.14 toks/s, output: 6.79 toks/s]Processed prompts:  29%|â–ˆâ–ˆâ–‰       | 52/177 [00:07<00:17,  7.12it/s, est. speed input: 16700.85 toks/s, output: 6.80 toks/s]Processed prompts:  30%|â–ˆâ–ˆâ–‰       | 53/177 [00:07<00:17,  7.14it/s, est. speed input: 16713.31 toks/s, output: 6.80 toks/s]Processed prompts:  31%|â–ˆâ–ˆâ–ˆ       | 54/177 [00:07<00:17,  7.15it/s, est. speed input: 16722.91 toks/s, output: 6.81 toks/s]Processed prompts:  31%|â–ˆâ–ˆâ–ˆ       | 55/177 [00:08<00:17,  7.13it/s, est. speed input: 16725.21 toks/s, output: 6.81 toks/s]Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 56/177 [00:08<00:17,  7.12it/s, est. speed input: 16734.07 toks/s, output: 6.82 toks/s]Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 57/177 [00:08<00:16,  7.10it/s, est. speed input: 16746.02 toks/s, output: 6.82 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 58/177 [00:08<00:16,  7.06it/s, est. speed input: 16754.08 toks/s, output: 6.82 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 59/177 [00:08<00:16,  7.09it/s, est. speed input: 16777.89 toks/s, output: 6.83 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 60/177 [00:08<00:16,  7.11it/s, est. speed input: 16789.20 toks/s, output: 6.84 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 61/177 [00:08<00:16,  7.11it/s, est. speed input: 16800.86 toks/s, output: 6.84 toks/s]Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 62/177 [00:09<00:16,  7.14it/s, est. speed input: 16813.02 toks/s, output: 6.85 toks/s]Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 63/177 [00:09<00:15,  7.14it/s, est. speed input: 16818.74 toks/s, output: 6.85 toks/s]Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 64/177 [00:09<00:16,  6.82it/s, est. speed input: 16789.80 toks/s, output: 6.84 toks/s]Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 65/177 [00:09<00:16,  6.90it/s, est. speed input: 16835.70 toks/s, output: 6.84 toks/s]Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 66/177 [00:09<00:15,  7.01it/s, est. speed input: 16857.06 toks/s, output: 6.85 toks/s]Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 67/177 [00:09<00:15,  7.04it/s, est. speed input: 16859.44 toks/s, output: 6.85 toks/s]Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 68/177 [00:09<00:15,  7.06it/s, est. speed input: 16866.61 toks/s, output: 6.86 toks/s]Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 69/177 [00:10<00:15,  6.92it/s, est. speed input: 16855.45 toks/s, output: 6.85 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 70/177 [00:10<00:15,  6.99it/s, est. speed input: 16886.70 toks/s, output: 6.86 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 71/177 [00:10<00:15,  6.85it/s, est. speed input: 16871.05 toks/s, output: 6.85 toks/s]Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 72/177 [00:10<00:15,  6.70it/s, est. speed input: 16870.92 toks/s, output: 6.84 toks/s]Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 73/177 [00:10<00:15,  6.55it/s, est. speed input: 16872.14 toks/s, output: 6.84 toks/s]Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 74/177 [00:10<00:15,  6.72it/s, est. speed input: 16912.72 toks/s, output: 6.84 toks/s]Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 75/177 [00:10<00:14,  6.82it/s, est. speed input: 16918.55 toks/s, output: 6.84 toks/s]Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 76/177 [00:11<00:14,  6.92it/s, est. speed input: 16930.19 toks/s, output: 6.85 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 77/177 [00:11<00:14,  7.00it/s, est. speed input: 16937.41 toks/s, output: 6.85 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 78/177 [00:11<00:14,  7.04it/s, est. speed input: 16943.32 toks/s, output: 6.85 toks/s]Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 79/177 [00:11<00:14,  6.80it/s, est. speed input: 16916.71 toks/s, output: 6.85 toks/s]Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 80/177 [00:11<00:14,  6.66it/s, est. speed input: 16925.27 toks/s, output: 6.84 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 81/177 [00:11<00:14,  6.78it/s, est. speed input: 16955.47 toks/s, output: 6.84 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 82/177 [00:11<00:13,  6.89it/s, est. speed input: 16964.46 toks/s, output: 6.85 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 83/177 [00:12<00:13,  6.96it/s, est. speed input: 16963.71 toks/s, output: 6.85 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 84/177 [00:12<00:13,  7.00it/s, est. speed input: 16967.70 toks/s, output: 6.85 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 85/177 [00:12<00:13,  7.05it/s, est. speed input: 16975.77 toks/s, output: 6.86 toks/s]Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 86/177 [00:12<00:12,  7.06it/s, est. speed input: 16975.33 toks/s, output: 6.86 toks/s]Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 87/177 [00:12<00:13,  6.81it/s, est. speed input: 16956.46 toks/s, output: 6.85 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 88/177 [00:12<00:12,  6.93it/s, est. speed input: 16991.96 toks/s, output: 6.86 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 89/177 [00:12<00:12,  6.98it/s, est. speed input: 16989.94 toks/s, output: 6.86 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 90/177 [00:13<00:12,  7.03it/s, est. speed input: 16996.41 toks/s, output: 6.86 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 91/177 [00:13<00:12,  6.88it/s, est. speed input: 16985.57 toks/s, output: 6.86 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 92/177 [00:13<00:12,  6.98it/s, est. speed input: 17007.74 toks/s, output: 6.86 toks/s]Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 93/177 [00:13<00:12,  6.99it/s, est. speed input: 17003.85 toks/s, output: 6.86 toks/s]Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 94/177 [00:13<00:11,  7.01it/s, est. speed input: 17007.22 toks/s, output: 6.86 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 95/177 [00:13<00:11,  7.05it/s, est. speed input: 17011.05 toks/s, output: 6.87 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 96/177 [00:13<00:11,  7.05it/s, est. speed input: 17012.68 toks/s, output: 6.87 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 97/177 [00:14<00:11,  7.09it/s, est. speed input: 17020.96 toks/s, output: 6.87 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 98/177 [00:14<00:11,  7.12it/s, est. speed input: 17022.46 toks/s, output: 6.88 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 99/177 [00:14<00:10,  7.11it/s, est. speed input: 17021.38 toks/s, output: 6.88 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 100/177 [00:14<00:11,  6.95it/s, est. speed input: 17011.62 toks/s, output: 6.87 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 101/177 [00:14<00:10,  6.99it/s, est. speed input: 17023.38 toks/s, output: 6.88 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 102/177 [00:14<00:10,  6.88it/s, est. speed input: 17014.90 toks/s, output: 6.87 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 103/177 [00:14<00:10,  6.98it/s, est. speed input: 17029.74 toks/s, output: 6.88 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 104/177 [00:15<00:10,  6.85it/s, est. speed input: 17015.53 toks/s, output: 6.87 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 105/177 [00:15<00:10,  6.93it/s, est. speed input: 17031.57 toks/s, output: 6.88 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 106/177 [00:15<00:10,  6.97it/s, est. speed input: 17031.81 toks/s, output: 6.88 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 107/177 [00:15<00:10,  6.97it/s, est. speed input: 17035.09 toks/s, output: 6.88 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 108/177 [00:15<00:09,  6.99it/s, est. speed input: 17037.24 toks/s, output: 6.88 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 109/177 [00:15<00:09,  7.04it/s, est. speed input: 17040.95 toks/s, output: 6.88 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 110/177 [00:15<00:09,  7.08it/s, est. speed input: 17041.17 toks/s, output: 6.89 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 111/177 [00:16<00:09,  7.09it/s, est. speed input: 17041.06 toks/s, output: 6.89 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 112/177 [00:16<00:09,  7.11it/s, est. speed input: 17043.56 toks/s, output: 6.89 toks/s]Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 113/177 [00:16<00:09,  7.09it/s, est. speed input: 17043.88 toks/s, output: 6.89 toks/s]Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 114/177 [00:16<00:08,  7.10it/s, est. speed input: 17048.38 toks/s, output: 6.89 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 115/177 [00:16<00:09,  6.77it/s, est. speed input: 17026.10 toks/s, output: 6.89 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 116/177 [00:16<00:08,  6.88it/s, est. speed input: 17048.41 toks/s, output: 6.89 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 117/177 [00:16<00:08,  6.93it/s, est. speed input: 17050.54 toks/s, output: 6.89 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 118/177 [00:17<00:08,  6.98it/s, est. speed input: 17055.81 toks/s, output: 6.89 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 119/177 [00:17<00:08,  6.76it/s, est. speed input: 17040.48 toks/s, output: 6.89 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 120/177 [00:17<00:08,  6.85it/s, est. speed input: 17060.75 toks/s, output: 6.89 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 121/177 [00:17<00:08,  6.81it/s, est. speed input: 17055.37 toks/s, output: 6.89 toks/s]Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 122/177 [00:17<00:08,  6.65it/s, est. speed input: 17051.74 toks/s, output: 6.88 toks/s]Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 123/177 [00:17<00:07,  6.80it/s, est. speed input: 17070.77 toks/s, output: 6.88 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 124/177 [00:18<00:07,  6.90it/s, est. speed input: 17070.49 toks/s, output: 6.88 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 125/177 [00:18<00:07,  6.96it/s, est. speed input: 17071.79 toks/s, output: 6.89 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 126/177 [00:18<00:07,  6.99it/s, est. speed input: 17071.95 toks/s, output: 6.89 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 127/177 [00:18<00:07,  6.98it/s, est. speed input: 17072.69 toks/s, output: 6.89 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 128/177 [00:18<00:06,  7.03it/s, est. speed input: 17079.09 toks/s, output: 6.89 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 129/177 [00:18<00:06,  7.07it/s, est. speed input: 17078.30 toks/s, output: 6.89 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 130/177 [00:18<00:06,  6.91it/s, est. speed input: 17068.46 toks/s, output: 6.89 toks/s]Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 131/177 [00:19<00:06,  6.95it/s, est. speed input: 17076.72 toks/s, output: 6.89 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 132/177 [00:19<00:06,  7.00it/s, est. speed input: 17080.59 toks/s, output: 6.89 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 133/177 [00:19<00:06,  6.72it/s, est. speed input: 17062.60 toks/s, output: 6.89 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 134/177 [00:19<00:06,  6.85it/s, est. speed input: 17082.84 toks/s, output: 6.89 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 135/177 [00:19<00:06,  6.93it/s, est. speed input: 17080.99 toks/s, output: 6.89 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 136/177 [00:19<00:05,  6.99it/s, est. speed input: 17080.16 toks/s, output: 6.89 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 137/177 [00:19<00:05,  7.03it/s, est. speed input: 17081.76 toks/s, output: 6.89 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 138/177 [00:20<00:05,  7.06it/s, est. speed input: 17081.32 toks/s, output: 6.89 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 139/177 [00:20<00:05,  7.09it/s, est. speed input: 17081.21 toks/s, output: 6.90 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 140/177 [00:20<00:05,  7.10it/s, est. speed input: 17082.59 toks/s, output: 6.90 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 141/177 [00:20<00:05,  7.07it/s, est. speed input: 17081.99 toks/s, output: 6.90 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 142/177 [00:20<00:05,  6.76it/s, est. speed input: 17065.12 toks/s, output: 6.89 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 143/177 [00:20<00:04,  6.92it/s, est. speed input: 17086.36 toks/s, output: 6.90 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 144/177 [00:20<00:04,  6.96it/s, est. speed input: 17082.35 toks/s, output: 6.90 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 145/177 [00:21<00:04,  7.00it/s, est. speed input: 17083.77 toks/s, output: 6.90 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 146/177 [00:21<00:04,  6.88it/s, est. speed input: 17080.72 toks/s, output: 6.90 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 147/177 [00:21<00:04,  6.94it/s, est. speed input: 17091.31 toks/s, output: 6.90 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 148/177 [00:21<00:04,  6.96it/s, est. speed input: 17091.94 toks/s, output: 6.90 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 149/177 [00:21<00:04,  6.97it/s, est. speed input: 17092.08 toks/s, output: 6.90 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 150/177 [00:21<00:03,  7.05it/s, est. speed input: 17094.57 toks/s, output: 6.90 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 151/177 [00:21<00:03,  7.05it/s, est. speed input: 17094.35 toks/s, output: 6.90 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 152/177 [00:22<00:03,  7.10it/s, est. speed input: 17099.47 toks/s, output: 6.90 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 153/177 [00:22<00:03,  7.08it/s, est. speed input: 17095.42 toks/s, output: 6.90 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 154/177 [00:22<00:03,  7.08it/s, est. speed input: 17101.16 toks/s, output: 6.91 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 155/177 [00:22<00:03,  7.09it/s, est. speed input: 17104.13 toks/s, output: 6.91 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 156/177 [00:22<00:03,  6.92it/s, est. speed input: 17094.93 toks/s, output: 6.90 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 157/177 [00:22<00:02,  6.96it/s, est. speed input: 17104.83 toks/s, output: 6.91 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 158/177 [00:22<00:02,  6.98it/s, est. speed input: 17105.90 toks/s, output: 6.91 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 159/177 [00:23<00:02,  7.01it/s, est. speed input: 17108.22 toks/s, output: 6.91 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 160/177 [00:23<00:02,  7.04it/s, est. speed input: 17111.80 toks/s, output: 6.91 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 161/177 [00:23<00:02,  6.90it/s, est. speed input: 17105.60 toks/s, output: 6.91 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 162/177 [00:23<00:02,  6.93it/s, est. speed input: 17115.14 toks/s, output: 6.91 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 163/177 [00:23<00:02,  6.69it/s, est. speed input: 17102.85 toks/s, output: 6.90 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 164/177 [00:23<00:01,  6.83it/s, est. speed input: 17120.25 toks/s, output: 6.90 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 165/177 [00:23<00:01,  6.89it/s, est. speed input: 17117.70 toks/s, output: 6.90 toks/s]Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 166/177 [00:24<00:01,  6.81it/s, est. speed input: 17111.69 toks/s, output: 6.90 toks/s]Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 167/177 [00:24<00:01,  6.87it/s, est. speed input: 17120.57 toks/s, output: 6.90 toks/s]Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 168/177 [00:24<00:01,  6.75it/s, est. speed input: 17114.33 toks/s, output: 6.90 toks/s]Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 169/177 [00:24<00:01,  6.86it/s, est. speed input: 17124.27 toks/s, output: 6.90 toks/s]Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 170/177 [00:24<00:01,  6.93it/s, est. speed input: 17124.65 toks/s, output: 6.90 toks/s]Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 171/177 [00:24<00:00,  6.99it/s, est. speed input: 17125.45 toks/s, output: 6.90 toks/s]Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 172/177 [00:24<00:00,  7.06it/s, est. speed input: 17128.25 toks/s, output: 6.91 toks/s]Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 173/177 [00:25<00:00,  7.11it/s, est. speed input: 17129.99 toks/s, output: 6.91 toks/s]Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 174/177 [00:25<00:00,  7.13it/s, est. speed input: 17129.64 toks/s, output: 6.91 toks/s]Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 175/177 [00:25<00:00,  7.14it/s, est. speed input: 17131.34 toks/s, output: 6.91 toks/s]Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 176/177 [00:25<00:00,  7.10it/s, est. speed input: 17129.38 toks/s, output: 6.91 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 177/177 [00:25<00:00,  6.95it/s, est. speed input: 17216.65 toks/s, output: 6.95 toks/s]
[PHASE 3] Task=causal_judgement, accuracy=57.63%, ft_time=16.28s, eval_time=26.06s
WARNING 01-28 20:42:58 tokenizer.py:191] No tokenizer found in /tmp/date_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/date_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/date_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:15<1:02:29, 15.69s/it, est. speed input: 73.11 toks/s, output: 0.19 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:15<00:00, 15.30it/s, est. speed input: 17458.83 toks/s, output: 45.89 toks/s]
[PHASE 3] Task=date_understanding, accuracy=58.33%, ft_time=14.65s, eval_time=15.99s
WARNING 01-28 20:43:14 tokenizer.py:191] No tokenizer found in /tmp/disambiguation_qa_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/disambiguation_qa_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/disambiguation_qa_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:11<47:02, 11.81s/it, est. speed input: 71.45 toks/s, output: 0.25 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:11<00:00, 20.31it/s, est. speed input: 17222.54 toks/s, output: 60.94 toks/s]
[PHASE 3] Task=disambiguation_qa, accuracy=63.75%, ft_time=14.92s, eval_time=12.07s
WARNING 01-28 20:43:26 tokenizer.py:191] No tokenizer found in /tmp/dyck_languages_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/dyck_languages_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/dyck_languages_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:06<24:52,  6.25s/it, est. speed input: 69.65 toks/s, output: 0.48 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:06<00:00, 38.41it/s, est. speed input: 17373.59 toks/s, output: 115.23 toks/s]
[PHASE 3] Task=dyck_languages, accuracy=53.33%, ft_time=15.48s, eval_time=6.40s
WARNING 01-28 20:43:33 tokenizer.py:191] No tokenizer found in /tmp/formal_fallacies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/formal_fallacies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/formal_fallacies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:00<02:11,  1.82it/s, est. speed input: 2496.88 toks/s, output: 1.82 toks/s]Processed prompts:   1%|â–         | 3/240 [00:00<00:52,  4.48it/s, est. speed input: 5435.92 toks/s, output: 3.91 toks/s]Processed prompts:   2%|â–         | 5/240 [00:00<00:38,  6.11it/s, est. speed input: 7009.57 toks/s, output: 5.09 toks/s]Processed prompts:   3%|â–Ž         | 8/240 [00:01<00:24,  9.50it/s, est. speed input: 9542.50 toks/s, output: 7.02 toks/s]Processed prompts:   5%|â–         | 11/240 [00:01<00:21, 10.75it/s, est. speed input: 10929.45 toks/s, output: 8.03 toks/s]Processed prompts:   5%|â–Œ         | 13/240 [00:01<00:20, 11.28it/s, est. speed input: 11620.80 toks/s, output: 8.52 toks/s]Processed prompts:   7%|â–‹         | 16/240 [00:01<00:16, 13.48it/s, est. speed input: 13056.62 toks/s, output: 9.54 toks/s]Processed prompts:   8%|â–Š         | 18/240 [00:01<00:16, 13.37it/s, est. speed input: 13460.98 toks/s, output: 9.83 toks/s]Processed prompts:   8%|â–Š         | 20/240 [00:02<00:18, 11.87it/s, est. speed input: 13362.52 toks/s, output: 9.76 toks/s]Processed prompts:   9%|â–‰         | 22/240 [00:02<00:18, 12.04it/s, est. speed input: 13635.08 toks/s, output: 9.96 toks/s]Processed prompts:  10%|â–ˆ         | 25/240 [00:02<00:15, 13.87it/s, est. speed input: 14406.55 toks/s, output: 10.54 toks/s]Processed prompts:  11%|â–ˆâ–        | 27/240 [00:02<00:15, 13.51it/s, est. speed input: 14599.89 toks/s, output: 10.67 toks/s]Processed prompts:  12%|â–ˆâ–        | 29/240 [00:02<00:17, 11.93it/s, est. speed input: 14444.40 toks/s, output: 10.55 toks/s]Processed prompts:  13%|â–ˆâ–Ž        | 31/240 [00:02<00:19, 10.98it/s, est. speed input: 14296.29 toks/s, output: 10.44 toks/s]Processed prompts:  14%|â–ˆâ–        | 34/240 [00:03<00:15, 13.08it/s, est. speed input: 14894.48 toks/s, output: 10.88 toks/s]Processed prompts:  15%|â–ˆâ–Œ        | 37/240 [00:03<00:15, 13.24it/s, est. speed input: 15116.35 toks/s, output: 11.06 toks/s]Processed prompts:  16%|â–ˆâ–‹        | 39/240 [00:03<00:15, 13.11it/s, est. speed input: 15223.19 toks/s, output: 11.13 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 42/240 [00:03<00:14, 13.32it/s, est. speed input: 15417.21 toks/s, output: 11.28 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 44/240 [00:03<00:16, 11.94it/s, est. speed input: 15257.41 toks/s, output: 11.16 toks/s]Processed prompts:  20%|â–ˆâ–‰        | 47/240 [00:04<00:15, 12.46it/s, est. speed input: 15420.42 toks/s, output: 11.29 toks/s]Processed prompts:  21%|â–ˆâ–ˆ        | 50/240 [00:04<00:13, 14.05it/s, est. speed input: 15798.64 toks/s, output: 11.57 toks/s]Processed prompts:  22%|â–ˆâ–ˆâ–       | 53/240 [00:04<00:13, 13.87it/s, est. speed input: 15935.86 toks/s, output: 11.66 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 55/240 [00:04<00:13, 13.64it/s, est. speed input: 15999.55 toks/s, output: 11.71 toks/s]Processed prompts:  24%|â–ˆâ–ˆâ–       | 58/240 [00:04<00:12, 15.15it/s, est. speed input: 16328.13 toks/s, output: 11.95 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 60/240 [00:05<00:12, 14.49it/s, est. speed input: 16375.88 toks/s, output: 11.98 toks/s]Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 62/240 [00:05<00:14, 12.54it/s, est. speed input: 16208.09 toks/s, output: 11.85 toks/s]Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 64/240 [00:05<00:13, 12.63it/s, est. speed input: 16250.58 toks/s, output: 11.88 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 67/240 [00:05<00:11, 14.47it/s, est. speed input: 16521.70 toks/s, output: 12.09 toks/s]Processed prompts:  29%|â–ˆâ–ˆâ–‰       | 69/240 [00:05<00:13, 12.58it/s, est. speed input: 16380.00 toks/s, output: 11.98 toks/s]Processed prompts:  30%|â–ˆâ–ˆâ–‰       | 71/240 [00:05<00:13, 12.67it/s, est. speed input: 16410.64 toks/s, output: 12.00 toks/s]Processed prompts:  31%|â–ˆâ–ˆâ–ˆ       | 74/240 [00:06<00:11, 14.50it/s, est. speed input: 16658.23 toks/s, output: 12.19 toks/s]Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 76/240 [00:06<00:11, 14.00it/s, est. speed input: 16686.33 toks/s, output: 12.20 toks/s]Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 78/240 [00:06<00:11, 13.71it/s, est. speed input: 16712.31 toks/s, output: 12.22 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 80/240 [00:06<00:11, 13.38it/s, est. speed input: 16726.15 toks/s, output: 12.23 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 82/240 [00:06<00:13, 11.80it/s, est. speed input: 16593.22 toks/s, output: 12.13 toks/s]Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 84/240 [00:06<00:12, 12.10it/s, est. speed input: 16624.30 toks/s, output: 12.15 toks/s]Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 87/240 [00:07<00:12, 12.62it/s, est. speed input: 16683.34 toks/s, output: 12.19 toks/s]Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 89/240 [00:07<00:13, 11.38it/s, est. speed input: 16548.66 toks/s, output: 12.09 toks/s]Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 92/240 [00:07<00:12, 12.08it/s, est. speed input: 16598.39 toks/s, output: 12.14 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 95/240 [00:07<00:11, 12.55it/s, est. speed input: 16652.44 toks/s, output: 12.18 toks/s]Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 98/240 [00:07<00:10, 14.20it/s, est. speed input: 16836.20 toks/s, output: 12.32 toks/s]Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 101/240 [00:08<00:09, 13.98it/s, est. speed input: 16883.20 toks/s, output: 12.35 toks/s]Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 103/240 [00:08<00:10, 13.64it/s, est. speed input: 16895.50 toks/s, output: 12.36 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 106/240 [00:08<00:09, 13.61it/s, est. speed input: 16935.51 toks/s, output: 12.39 toks/s]Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 108/240 [00:08<00:09, 13.44it/s, est. speed input: 16949.89 toks/s, output: 12.40 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 111/240 [00:08<00:09, 13.50it/s, est. speed input: 16992.81 toks/s, output: 12.43 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 113/240 [00:09<00:10, 12.05it/s, est. speed input: 16880.70 toks/s, output: 12.35 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 116/240 [00:09<00:09, 13.77it/s, est. speed input: 17024.48 toks/s, output: 12.46 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 119/240 [00:09<00:07, 15.16it/s, est. speed input: 17175.22 toks/s, output: 12.57 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 121/240 [00:09<00:09, 13.04it/s, est. speed input: 17069.53 toks/s, output: 12.49 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 123/240 [00:09<00:09, 11.75it/s, est. speed input: 16968.29 toks/s, output: 12.41 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 126/240 [00:10<00:08, 13.64it/s, est. speed input: 17109.09 toks/s, output: 12.52 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 129/240 [00:10<00:08, 13.63it/s, est. speed input: 17137.05 toks/s, output: 12.54 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 131/240 [00:10<00:08, 12.13it/s, est. speed input: 17038.57 toks/s, output: 12.47 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 134/240 [00:10<00:07, 13.90it/s, est. speed input: 17169.68 toks/s, output: 12.57 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 137/240 [00:10<00:06, 15.30it/s, est. speed input: 17299.37 toks/s, output: 12.66 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 139/240 [00:11<00:07, 13.19it/s, est. speed input: 17205.35 toks/s, output: 12.59 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 141/240 [00:11<00:08, 11.82it/s, est. speed input: 17112.90 toks/s, output: 12.52 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 144/240 [00:11<00:07, 13.68it/s, est. speed input: 17234.60 toks/s, output: 12.62 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 147/240 [00:11<00:06, 15.10it/s, est. speed input: 17356.25 toks/s, output: 12.70 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 149/240 [00:11<00:06, 14.48it/s, est. speed input: 17360.22 toks/s, output: 12.71 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 151/240 [00:11<00:06, 14.06it/s, est. speed input: 17367.70 toks/s, output: 12.71 toks/s]Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 153/240 [00:12<00:06, 13.72it/s, est. speed input: 17375.04 toks/s, output: 12.71 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 155/240 [00:12<00:06, 13.45it/s, est. speed input: 17378.18 toks/s, output: 12.71 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 157/240 [00:12<00:06, 13.25it/s, est. speed input: 17383.12 toks/s, output: 12.71 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 159/240 [00:12<00:06, 13.12it/s, est. speed input: 17383.85 toks/s, output: 12.71 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 161/240 [00:12<00:06, 13.03it/s, est. speed input: 17385.42 toks/s, output: 12.71 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 163/240 [00:12<00:05, 12.98it/s, est. speed input: 17389.31 toks/s, output: 12.72 toks/s]Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 165/240 [00:12<00:05, 12.96it/s, est. speed input: 17392.36 toks/s, output: 12.72 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 167/240 [00:13<00:05, 12.91it/s, est. speed input: 17393.75 toks/s, output: 12.72 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 169/240 [00:13<00:05, 12.90it/s, est. speed input: 17397.72 toks/s, output: 12.72 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 171/240 [00:13<00:05, 12.88it/s, est. speed input: 17405.03 toks/s, output: 12.72 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 173/240 [00:13<00:05, 12.86it/s, est. speed input: 17408.72 toks/s, output: 12.72 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 175/240 [00:13<00:05, 12.86it/s, est. speed input: 17412.35 toks/s, output: 12.73 toks/s]Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 177/240 [00:13<00:04, 12.82it/s, est. speed input: 17415.67 toks/s, output: 12.73 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 179/240 [00:14<00:05, 11.37it/s, est. speed input: 17334.73 toks/s, output: 12.67 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 181/240 [00:14<00:05, 11.71it/s, est. speed input: 17337.50 toks/s, output: 12.67 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 184/240 [00:14<00:04, 12.38it/s, est. speed input: 17358.45 toks/s, output: 12.68 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 186/240 [00:14<00:04, 12.50it/s, est. speed input: 17363.14 toks/s, output: 12.68 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 189/240 [00:14<00:03, 14.38it/s, est. speed input: 17458.70 toks/s, output: 12.75 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 191/240 [00:14<00:03, 14.00it/s, est. speed input: 17462.04 toks/s, output: 12.75 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 193/240 [00:15<00:03, 13.63it/s, est. speed input: 17461.13 toks/s, output: 12.75 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 195/240 [00:15<00:03, 11.91it/s, est. speed input: 17392.46 toks/s, output: 12.70 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 197/240 [00:15<00:03, 12.15it/s, est. speed input: 17390.11 toks/s, output: 12.70 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 200/240 [00:15<00:02, 14.13it/s, est. speed input: 17479.31 toks/s, output: 12.77 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 202/240 [00:15<00:02, 13.77it/s, est. speed input: 17481.44 toks/s, output: 12.77 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 204/240 [00:15<00:02, 13.55it/s, est. speed input: 17488.78 toks/s, output: 12.77 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 206/240 [00:16<00:02, 13.36it/s, est. speed input: 17491.65 toks/s, output: 12.77 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 208/240 [00:16<00:02, 13.21it/s, est. speed input: 17493.40 toks/s, output: 12.77 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 210/240 [00:16<00:02, 13.12it/s, est. speed input: 17495.04 toks/s, output: 12.77 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 212/240 [00:16<00:02, 13.04it/s, est. speed input: 17495.19 toks/s, output: 12.77 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 214/240 [00:16<00:02, 12.98it/s, est. speed input: 17496.89 toks/s, output: 12.77 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 216/240 [00:16<00:02, 11.50it/s, est. speed input: 17429.94 toks/s, output: 12.73 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 218/240 [00:17<00:01, 11.85it/s, est. speed input: 17431.35 toks/s, output: 12.73 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 221/240 [00:17<00:01, 13.94it/s, est. speed input: 17508.18 toks/s, output: 12.78 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 223/240 [00:17<00:01, 13.62it/s, est. speed input: 17512.75 toks/s, output: 12.79 toks/s]Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 225/240 [00:17<00:01, 13.37it/s, est. speed input: 17511.83 toks/s, output: 12.78 toks/s]Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 227/240 [00:17<00:01, 11.76it/s, est. speed input: 17453.06 toks/s, output: 12.74 toks/s]Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 229/240 [00:18<00:01, 10.80it/s, est. speed input: 17389.37 toks/s, output: 12.69 toks/s]Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 232/240 [00:18<00:00, 12.96it/s, est. speed input: 17462.78 toks/s, output: 12.75 toks/s]Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 235/240 [00:18<00:00, 13.14it/s, est. speed input: 17474.26 toks/s, output: 12.76 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:18<00:00, 12.96it/s, est. speed input: 17753.37 toks/s, output: 12.96 toks/s]
[PHASE 3] Task=formal_fallacies, accuracy=59.17%, ft_time=14.82s, eval_time=18.96s
WARNING 01-28 20:43:52 tokenizer.py:191] No tokenizer found in /tmp/geometric_shapes_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/geometric_shapes_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/geometric_shapes_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:18<1:12:43, 18.26s/it, est. speed input: 73.06 toks/s, output: 0.16 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:18<00:00, 13.14it/s, est. speed input: 17763.38 toks/s, output: 39.43 toks/s]
[PHASE 3] Task=geometric_shapes, accuracy=46.25%, ft_time=14.60s, eval_time=18.60s
WARNING 01-28 20:44:10 tokenizer.py:191] No tokenizer found in /tmp/hyperbaton_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/hyperbaton_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/hyperbaton_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:05<22:25,  5.63s/it, est. speed input: 72.65 toks/s, output: 0.53 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:05<00:00, 42.62it/s, est. speed input: 17283.07 toks/s, output: 127.85 toks/s]
[PHASE 3] Task=hyperbaton, accuracy=71.67%, ft_time=14.87s, eval_time=5.77s
WARNING 01-28 20:44:16 tokenizer.py:191] No tokenizer found in /tmp/logical_deduction_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/logical_deduction_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/logical_deduction_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:22<1:28:15, 22.16s/it, est. speed input: 71.54 toks/s, output: 0.14 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:22<00:00, 10.83it/s, est. speed input: 17176.11 toks/s, output: 32.49 toks/s]
[PHASE 3] Task=logical_deduction_five_objects, accuracy=40.83%, ft_time=15.23s, eval_time=22.63s
WARNING 01-28 20:44:39 tokenizer.py:191] No tokenizer found in /tmp/logical_deduction_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/logical_deduction_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/logical_deduction_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:26<1:44:39, 26.27s/it, est. speed input: 86.51 toks/s, output: 0.11 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 197/240 [00:32<00:05,  8.10it/s, est. speed input: 13740.60 toks/s, output: 18.37 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:32<00:00,  7.46it/s, est. speed input: 16744.38 toks/s, output: 22.38 toks/s]
[PHASE 3] Task=logical_deduction_seven_objects, accuracy=52.50%, ft_time=15.34s, eval_time=32.84s
WARNING 01-28 20:45:12 tokenizer.py:191] No tokenizer found in /tmp/logical_deduction_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/logical_deduction_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/logical_deduction_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:15<1:03:03, 15.83s/it, est. speed input: 73.71 toks/s, output: 0.19 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:15<00:00, 15.16it/s, est. speed input: 17598.58 toks/s, output: 45.47 toks/s]
[PHASE 3] Task=logical_deduction_three_objects, accuracy=73.33%, ft_time=14.30s, eval_time=16.19s
WARNING 01-28 20:45:28 tokenizer.py:191] No tokenizer found in /tmp/movie_recommendation_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/movie_recommendation_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/movie_recommendation_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:09<39:00,  9.79s/it, est. speed input: 71.69 toks/s, output: 0.31 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:09<00:00, 24.50it/s, est. speed input: 17120.22 toks/s, output: 73.51 toks/s]
[PHASE 3] Task=movie_recommendation, accuracy=76.25%, ft_time=14.01s, eval_time=10.01s
WARNING 01-28 20:45:38 tokenizer.py:191] No tokenizer found in /tmp/multistep_arithmetic_two_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/multistep_arithmetic_two_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/multistep_arithmetic_two_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:05<21:07,  5.30s/it, est. speed input: 72.25 toks/s, output: 0.57 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:05<00:00, 45.25it/s, est. speed input: 17309.57 toks/s, output: 135.75 toks/s]
[PHASE 3] Task=multistep_arithmetic_two, accuracy=2.08%, ft_time=14.29s, eval_time=5.41s
WARNING 01-28 20:45:43 tokenizer.py:191] No tokenizer found in /tmp/navigate_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/navigate_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/navigate_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:00<02:06,  1.89it/s, est. speed input: 1120.74 toks/s, output: 1.89 toks/s]Processed prompts:   3%|â–Ž         | 7/240 [00:00<00:19, 11.77it/s, est. speed input: 5728.89 toks/s, output: 9.62 toks/s]Processed prompts:   5%|â–Œ         | 13/240 [00:00<00:12, 17.89it/s, est. speed input: 8390.92 toks/s, output: 14.02 toks/s]Processed prompts:   8%|â–Š         | 19/240 [00:01<00:10, 21.86it/s, est. speed input: 10123.50 toks/s, output: 16.88 toks/s]Processed prompts:  10%|â–ˆ         | 25/240 [00:01<00:08, 24.54it/s, est. speed input: 11384.83 toks/s, output: 18.90 toks/s]Processed prompts:  13%|â–ˆâ–Ž        | 31/240 [00:01<00:07, 26.21it/s, est. speed input: 12242.46 toks/s, output: 20.34 toks/s]Processed prompts:  15%|â–ˆâ–Œ        | 37/240 [00:01<00:07, 27.48it/s, est. speed input: 12905.96 toks/s, output: 21.50 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 43/240 [00:01<00:06, 28.34it/s, est. speed input: 13460.62 toks/s, output: 22.40 toks/s]Processed prompts:  20%|â–ˆâ–ˆ        | 49/240 [00:02<00:06, 29.08it/s, est. speed input: 13922.98 toks/s, output: 23.18 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 55/240 [00:02<00:06, 29.07it/s, est. speed input: 14238.66 toks/s, output: 23.70 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 61/240 [00:02<00:06, 29.31it/s, est. speed input: 14523.74 toks/s, output: 24.19 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 67/240 [00:02<00:05, 29.96it/s, est. speed input: 14850.04 toks/s, output: 24.71 toks/s]Processed prompts:  30%|â–ˆâ–ˆâ–ˆ       | 73/240 [00:02<00:05, 30.44it/s, est. speed input: 15107.47 toks/s, output: 25.16 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 79/240 [00:03<00:05, 30.22it/s, est. speed input: 15279.94 toks/s, output: 25.46 toks/s]Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 85/240 [00:03<00:05, 29.91it/s, est. speed input: 15413.55 toks/s, output: 25.69 toks/s]Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 91/240 [00:03<00:05, 29.75it/s, est. speed input: 15548.05 toks/s, output: 25.90 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 97/240 [00:03<00:04, 29.81it/s, est. speed input: 15686.21 toks/s, output: 26.12 toks/s]Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 103/240 [00:03<00:04, 29.89it/s, est. speed input: 15799.58 toks/s, output: 26.32 toks/s]Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 109/240 [00:04<00:04, 29.83it/s, est. speed input: 15899.75 toks/s, output: 26.49 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 115/240 [00:04<00:04, 30.13it/s, est. speed input: 16014.80 toks/s, output: 26.69 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 121/240 [00:04<00:03, 30.37it/s, est. speed input: 16124.14 toks/s, output: 26.87 toks/s]Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 127/240 [00:04<00:03, 30.17it/s, est. speed input: 16193.64 toks/s, output: 26.99 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 133/240 [00:04<00:03, 30.13it/s, est. speed input: 16264.09 toks/s, output: 27.12 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 139/240 [00:05<00:03, 30.02it/s, est. speed input: 16325.39 toks/s, output: 27.22 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 145/240 [00:05<00:03, 29.88it/s, est. speed input: 16387.61 toks/s, output: 27.31 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 151/240 [00:05<00:02, 29.71it/s, est. speed input: 16431.51 toks/s, output: 27.38 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 157/240 [00:05<00:02, 29.65it/s, est. speed input: 16475.75 toks/s, output: 27.46 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 163/240 [00:05<00:02, 29.70it/s, est. speed input: 16531.45 toks/s, output: 27.54 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 169/240 [00:06<00:02, 29.80it/s, est. speed input: 16582.80 toks/s, output: 27.62 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 175/240 [00:06<00:02, 30.14it/s, est. speed input: 16645.47 toks/s, output: 27.72 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 181/240 [00:06<00:01, 29.88it/s, est. speed input: 16676.31 toks/s, output: 27.77 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 187/240 [00:06<00:01, 30.22it/s, est. speed input: 16729.64 toks/s, output: 27.87 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 193/240 [00:06<00:01, 30.49it/s, est. speed input: 16788.14 toks/s, output: 27.96 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 199/240 [00:07<00:01, 30.81it/s, est. speed input: 16840.26 toks/s, output: 28.06 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 205/240 [00:07<00:01, 30.80it/s, est. speed input: 16882.04 toks/s, output: 28.13 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 211/240 [00:07<00:00, 30.32it/s, est. speed input: 16895.14 toks/s, output: 28.16 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 217/240 [00:07<00:00, 30.56it/s, est. speed input: 16936.27 toks/s, output: 28.23 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 223/240 [00:07<00:00, 30.78it/s, est. speed input: 16984.73 toks/s, output: 28.31 toks/s]Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 229/240 [00:08<00:00, 30.47it/s, est. speed input: 17005.68 toks/s, output: 28.35 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:08<00:00, 29.64it/s, est. speed input: 17781.89 toks/s, output: 29.65 toks/s]
[PHASE 3] Task=navigate, accuracy=65.00%, ft_time=14.21s, eval_time=8.29s
WARNING 01-28 20:45:52 tokenizer.py:191] No tokenizer found in /tmp/object_counting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/object_counting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/object_counting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:06<24:36,  6.18s/it, est. speed input: 70.75 toks/s, output: 0.32 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:06<00:00, 38.84it/s, est. speed input: 17499.44 toks/s, output: 77.69 toks/s]
[PHASE 3] Task=object_counting, accuracy=59.58%, ft_time=14.47s, eval_time=6.32s
WARNING 01-28 20:45:58 tokenizer.py:191] No tokenizer found in /tmp/penguins_in_a_table_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/penguins_in_a_table_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/penguins_in_a_table_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/136 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/136 [00:14<33:02, 14.68s/it, est. speed input: 126.76 toks/s, output: 0.20 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136/136 [00:14<00:00,  9.26it/s, est. speed input: 17246.84 toks/s, output: 27.79 toks/s]
[PHASE 3] Task=penguins_in_a_table, accuracy=53.68%, ft_time=14.93s, eval_time=14.97s
WARNING 01-28 20:46:13 tokenizer.py:191] No tokenizer found in /tmp/reasoning_about_colored_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/reasoning_about_colored_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/reasoning_about_colored_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:19<1:16:17, 19.15s/it, est. speed input: 72.10 toks/s, output: 0.16 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:19<00:00, 12.53it/s, est. speed input: 17313.75 toks/s, output: 37.59 toks/s]
[PHASE 3] Task=reasoning_about_colored_objects, accuracy=50.83%, ft_time=14.65s, eval_time=19.56s
WARNING 01-28 20:46:32 tokenizer.py:191] No tokenizer found in /tmp/ruin_names_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/ruin_names_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/ruin_names_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:09<37:31,  9.42s/it, est. speed input: 72.82 toks/s, output: 0.32 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:09<00:00, 25.47it/s, est. speed input: 17435.05 toks/s, output: 76.41 toks/s]
[PHASE 3] Task=ruin_names, accuracy=80.83%, ft_time=14.65s, eval_time=9.64s
WARNING 01-28 20:46:42 tokenizer.py:191] No tokenizer found in /tmp/salient_translation_error_detection_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/salient_translation_error_detection_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/salient_translation_error_detection_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:25<1:43:27, 25.97s/it, est. speed input: 113.69 toks/s, output: 0.12 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 148/240 [00:42<00:21,  4.27it/s, est. speed input: 10433.35 toks/s, output: 10.49 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:42<00:00,  5.67it/s, est. speed input: 16916.22 toks/s, output: 17.00 toks/s]
[PHASE 3] Task=salient_translation_error_detection, accuracy=45.42%, ft_time=15.59s, eval_time=43.23s
WARNING 01-28 20:47:25 tokenizer.py:191] No tokenizer found in /tmp/snarks_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/snarks_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/snarks_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/168 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/168 [00:06<16:50,  6.05s/it, est. speed input: 97.51 toks/s, output: 0.50 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:06<00:00, 27.76it/s, est. speed input: 16768.63 toks/s, output: 83.28 toks/s]
[PHASE 3] Task=snarks, accuracy=77.38%, ft_time=14.54s, eval_time=6.21s
WARNING 01-28 20:47:32 tokenizer.py:191] No tokenizer found in /tmp/sports_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/sports_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/sports_understanding_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:00<02:15,  1.76it/s, est. speed input: 489.55 toks/s, output: 1.76 toks/s]Processed prompts:   6%|â–‹         | 15/240 [00:00<00:09, 24.20it/s, est. speed input: 5353.99 toks/s, output: 19.29 toks/s]Processed prompts:  12%|â–ˆâ–        | 29/240 [00:00<00:05, 38.23it/s, est. speed input: 8155.35 toks/s, output: 29.35 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 43/240 [00:01<00:04, 46.36it/s, est. speed input: 9872.87 toks/s, output: 35.48 toks/s]Processed prompts:  24%|â–ˆâ–ˆâ–       | 57/240 [00:01<00:03, 52.16it/s, est. speed input: 11094.12 toks/s, output: 39.91 toks/s]Processed prompts:  30%|â–ˆâ–ˆâ–‰       | 71/240 [00:01<00:02, 56.66it/s, est. speed input: 12055.54 toks/s, output: 43.36 toks/s]Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 85/240 [00:01<00:02, 59.63it/s, est. speed input: 12795.09 toks/s, output: 45.99 toks/s]Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 99/240 [00:02<00:02, 61.30it/s, est. speed input: 13345.48 toks/s, output: 47.98 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 113/240 [00:02<00:02, 62.07it/s, est. speed input: 13766.95 toks/s, output: 49.50 toks/s]Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 127/240 [00:02<00:01, 63.08it/s, est. speed input: 14146.76 toks/s, output: 50.87 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 141/240 [00:02<00:01, 63.91it/s, est. speed input: 14480.98 toks/s, output: 52.04 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 155/240 [00:02<00:01, 64.58it/s, est. speed input: 14765.07 toks/s, output: 53.06 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 169/240 [00:03<00:01, 64.68it/s, est. speed input: 14992.22 toks/s, output: 53.88 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 183/240 [00:03<00:00, 64.59it/s, est. speed input: 15183.06 toks/s, output: 54.56 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 197/240 [00:03<00:00, 64.91it/s, est. speed input: 15373.35 toks/s, output: 55.22 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 211/240 [00:03<00:00, 65.30it/s, est. speed input: 15546.34 toks/s, output: 55.84 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:03<00:00, 62.51it/s, est. speed input: 17394.89 toks/s, output: 62.51 toks/s]
[PHASE 3] Task=sports_understanding, accuracy=71.25%, ft_time=14.30s, eval_time=3.95s
WARNING 01-28 20:47:35 tokenizer.py:191] No tokenizer found in /tmp/temporal_sequences_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/temporal_sequences_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/temporal_sequences_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:25<1:41:09, 25.40s/it, est. speed input: 73.83 toks/s, output: 0.12 toks/s]Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 233/240 [00:26<00:00, 12.46it/s, est. speed input: 16789.63 toks/s, output: 26.58 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:26<00:00,  9.13it/s, est. speed input: 17294.90 toks/s, output: 27.38 toks/s]
[PHASE 3] Task=temporal_sequences, accuracy=100.00%, ft_time=14.79s, eval_time=26.85s
WARNING 01-28 20:48:02 tokenizer.py:191] No tokenizer found in /tmp/tracking_shuffled_objects_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/tracking_shuffled_objects_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/tracking_shuffled_objects_five_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:25<1:41:49, 25.56s/it, est. speed input: 71.90 toks/s, output: 0.12 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:25<00:00,  9.39it/s, est. speed input: 17082.71 toks/s, output: 28.16 toks/s]
[PHASE 3] Task=tracking_shuffled_objects_five_objects, accuracy=19.58%, ft_time=14.75s, eval_time=26.15s
WARNING 01-28 20:48:28 tokenizer.py:191] No tokenizer found in /tmp/tracking_shuffled_objects_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/tracking_shuffled_objects_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/tracking_shuffled_objects_seven_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:26<1:44:45, 26.30s/it, est. speed input: 92.09 toks/s, output: 0.11 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 184/240 [00:34<00:08,  6.91it/s, est. speed input: 12834.27 toks/s, output: 16.02 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:34<00:00,  6.96it/s, est. speed input: 16739.07 toks/s, output: 20.89 toks/s]
[PHASE 3] Task=tracking_shuffled_objects_seven_objects, accuracy=16.67%, ft_time=15.40s, eval_time=35.20s
WARNING 01-28 20:49:04 tokenizer.py:191] No tokenizer found in /tmp/tracking_shuffled_objects_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/tracking_shuffled_objects_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/tracking_shuffled_objects_three_objects_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:18<1:11:55, 18.06s/it, est. speed input: 73.54 toks/s, output: 0.17 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:18<00:00, 13.29it/s, est. speed input: 17588.47 toks/s, output: 39.87 toks/s]
[PHASE 3] Task=tracking_shuffled_objects_three_objects, accuracy=38.33%, ft_time=15.13s, eval_time=18.49s
WARNING 01-28 20:49:22 tokenizer.py:191] No tokenizer found in /tmp/web_of_lies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/web_of_lies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/web_of_lies_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:00<02:06,  1.89it/s, est. speed input: 1118.84 toks/s, output: 1.89 toks/s]Processed prompts:   3%|â–Ž         | 7/240 [00:00<00:19, 11.99it/s, est. speed input: 5758.26 toks/s, output: 9.75 toks/s]Processed prompts:   5%|â–Œ         | 13/240 [00:00<00:12, 18.45it/s, est. speed input: 8445.21 toks/s, output: 14.35 toks/s]Processed prompts:   8%|â–Š         | 19/240 [00:01<00:09, 22.48it/s, est. speed input: 10164.91 toks/s, output: 17.27 toks/s]Processed prompts:  10%|â–ˆ         | 25/240 [00:01<00:08, 24.94it/s, est. speed input: 11323.86 toks/s, output: 19.24 toks/s]Processed prompts:  13%|â–ˆâ–Ž        | 31/240 [00:01<00:07, 26.90it/s, est. speed input: 12234.80 toks/s, output: 20.79 toks/s]Processed prompts:  15%|â–ˆâ–Œ        | 37/240 [00:01<00:07, 28.26it/s, est. speed input: 12937.66 toks/s, output: 22.00 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 43/240 [00:01<00:06, 29.40it/s, est. speed input: 13535.79 toks/s, output: 23.01 toks/s]Processed prompts:  20%|â–ˆâ–ˆ        | 49/240 [00:02<00:06, 29.95it/s, est. speed input: 13985.84 toks/s, output: 23.77 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 55/240 [00:02<00:06, 30.28it/s, est. speed input: 14346.30 toks/s, output: 24.39 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 61/240 [00:02<00:05, 30.61it/s, est. speed input: 14667.23 toks/s, output: 24.94 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 67/240 [00:02<00:05, 30.96it/s, est. speed input: 14950.36 toks/s, output: 25.43 toks/s]Processed prompts:  30%|â–ˆâ–ˆâ–ˆ       | 73/240 [00:02<00:05, 31.15it/s, est. speed input: 15188.36 toks/s, output: 25.84 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 79/240 [00:03<00:05, 31.24it/s, est. speed input: 15399.89 toks/s, output: 26.20 toks/s]Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 85/240 [00:03<00:04, 31.06it/s, est. speed input: 15555.48 toks/s, output: 26.47 toks/s]Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 91/240 [00:03<00:04, 31.01it/s, est. speed input: 15701.86 toks/s, output: 26.72 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 97/240 [00:03<00:04, 31.30it/s, est. speed input: 15861.85 toks/s, output: 27.00 toks/s]Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 103/240 [00:03<00:04, 31.31it/s, est. speed input: 15993.78 toks/s, output: 27.22 toks/s]Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 109/240 [00:03<00:04, 31.37it/s, est. speed input: 16112.68 toks/s, output: 27.42 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 115/240 [00:04<00:04, 29.87it/s, est. speed input: 16094.84 toks/s, output: 27.39 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 121/240 [00:04<00:03, 30.20it/s, est. speed input: 16186.22 toks/s, output: 27.55 toks/s]Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 128/240 [00:04<00:03, 32.10it/s, est. speed input: 16407.60 toks/s, output: 27.93 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 134/240 [00:04<00:03, 31.98it/s, est. speed input: 16496.36 toks/s, output: 28.08 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 140/240 [00:04<00:03, 31.65it/s, est. speed input: 16563.76 toks/s, output: 28.19 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 146/240 [00:05<00:02, 31.52it/s, est. speed input: 16629.60 toks/s, output: 28.30 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 152/240 [00:05<00:02, 31.41it/s, est. speed input: 16689.81 toks/s, output: 28.41 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 158/240 [00:05<00:02, 31.34it/s, est. speed input: 16747.14 toks/s, output: 28.50 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 164/240 [00:05<00:02, 31.42it/s, est. speed input: 16806.21 toks/s, output: 28.61 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 170/240 [00:05<00:02, 31.34it/s, est. speed input: 16855.00 toks/s, output: 28.69 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 176/240 [00:06<00:02, 31.36it/s, est. speed input: 16907.46 toks/s, output: 28.77 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 182/240 [00:06<00:01, 31.31it/s, est. speed input: 16950.41 toks/s, output: 28.85 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 188/240 [00:06<00:01, 31.28it/s, est. speed input: 16991.20 toks/s, output: 28.92 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 194/240 [00:06<00:01, 31.42it/s, est. speed input: 17038.73 toks/s, output: 29.00 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 200/240 [00:06<00:01, 31.34it/s, est. speed input: 17073.06 toks/s, output: 29.06 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 206/240 [00:07<00:01, 31.31it/s, est. speed input: 17105.90 toks/s, output: 29.12 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 212/240 [00:07<00:00, 31.25it/s, est. speed input: 17135.47 toks/s, output: 29.17 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 218/240 [00:07<00:00, 31.29it/s, est. speed input: 17168.56 toks/s, output: 29.23 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 224/240 [00:07<00:00, 31.38it/s, est. speed input: 17203.14 toks/s, output: 29.29 toks/s]Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 230/240 [00:07<00:00, 32.70it/s, est. speed input: 17288.99 toks/s, output: 29.43 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:07<00:00, 30.65it/s, est. speed input: 18001.19 toks/s, output: 30.65 toks/s]
[PHASE 3] Task=web_of_lies, accuracy=47.92%, ft_time=14.77s, eval_time=8.02s
WARNING 01-28 20:49:30 tokenizer.py:191] No tokenizer found in /tmp/word_sorting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter, using base model tokenizer instead. (Exception: Can't load tokenizer for '/tmp/word_sorting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/tmp/word_sorting_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4_adapter' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.)
Processed prompts:   0%|          | 0/240 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/240 [00:08<34:50,  8.75s/it, est. speed input: 67.12 toks/s, output: 0.57 toks/s]Processed prompts:   2%|â–         | 5/240 [00:08<05:14,  1.34s/it, est. speed input: 329.81 toks/s, output: 3.25 toks/s]Processed prompts:   7%|â–‹         | 16/240 [00:09<01:11,  3.13it/s, est. speed input: 1038.98 toks/s, output: 12.46 toks/s]Processed prompts:  13%|â–ˆâ–Ž        | 31/240 [00:09<00:27,  7.47it/s, est. speed input: 1983.59 toks/s, output: 27.54 toks/s]Processed prompts:  16%|â–ˆâ–Œ        | 38/240 [00:09<00:20,  9.84it/s, est. speed input: 2396.01 toks/s, output: 36.28 toks/s]Processed prompts:  22%|â–ˆâ–ˆâ–       | 53/240 [00:09<00:10, 17.05it/s, est. speed input: 3298.95 toks/s, output: 57.17 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 61/240 [00:09<00:08, 20.99it/s, est. speed input: 3748.89 toks/s, output: 69.24 toks/s]Processed prompts:  30%|â–ˆâ–ˆâ–‰       | 71/240 [00:09<00:06, 27.33it/s, est. speed input: 4312.14 toks/s, output: 86.32 toks/s]Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 85/240 [00:09<00:03, 38.82it/s, est. speed input: 5109.25 toks/s, output: 113.12 toks/s]Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 103/240 [00:10<00:02, 56.54it/s, est. speed input: 6134.84 toks/s, output: 150.66 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 114/240 [00:10<00:01, 64.31it/s, est. speed input: 6728.05 toks/s, output: 174.66 toks/s]Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 127/240 [00:10<00:01, 76.17it/s, est. speed input: 7433.85 toks/s, output: 205.70 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 150/240 [00:10<00:00, 101.51it/s, est. speed input: 8694.66 toks/s, output: 264.83 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 164/240 [00:10<00:00, 105.37it/s, est. speed input: 9414.59 toks/s, output: 304.38 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 180/240 [00:10<00:00, 116.69it/s, est. speed input: 10254.11 toks/s, output: 353.47 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 213/240 [00:10<00:00, 166.53it/s, est. speed input: 12066.80 toks/s, output: 466.49 toks/s]Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 233/240 [00:10<00:00, 172.35it/s, est. speed input: 13102.92 toks/s, output: 539.84 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:10<00:00, 22.12it/s, est. speed input: 13435.70 toks/s, output: 568.61 toks/s] 
[PHASE 3] Task=word_sorting, accuracy=47.92%, ft_time=14.13s, eval_time=11.06s

[TTT] Results saved to logs/current/20250128_TTT_exp_10_masked_inputs_text_completion_dataset_1_False_False_5_False_1_4_1e-4_64_64_0.05_4.json.
[TTT] Done. Goodbye!
[rank0]:[W128 20:49:43.025999111 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
