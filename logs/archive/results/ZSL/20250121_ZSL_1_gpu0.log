INFO 01-21 21:12:26 config.py:478] This model supports multiple tasks: {'generate', 'reward', 'embed', 'classify', 'score'}. Defaulting to 'generate'.
INFO 01-21 21:12:26 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='/data/cl/u/adamz/Models/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='/data/cl/u/adamz/Models/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/data/cl/u/adamz/Models/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 01-21 21:12:28 selector.py:120] Using Flash Attention backend.
INFO 01-21 21:12:29 model_runner.py:1092] Starting to load model /data/cl/u/adamz/Models/Llama-3.1-8B-Instruct...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:50<02:32, 50.69s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [01:41<01:41, 50.67s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [02:32<00:50, 50.98s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:45<00:00, 35.80s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:45<00:00, 41.31s/it]

INFO 01-21 21:15:15 model_runner.py:1097] Loading model weights took 14.9888 GB
INFO 01-21 21:15:16 worker.py:241] Memory profiling takes 1.67 seconds
INFO 01-21 21:15:16 worker.py:241] the current vLLM instance can use total_gpu_memory (44.45GiB) x gpu_memory_utilization (0.90) = 40.00GiB
INFO 01-21 21:15:16 worker.py:241] model weights take 14.99GiB; non_torch_memory takes 0.07GiB; PyTorch activation peak memory takes 1.23GiB; the rest of the memory reserved for KV Cache is 23.71GiB.
INFO 01-21 21:15:17 gpu_executor.py:76] # GPU blocks: 12142, # CPU blocks: 2048
INFO 01-21 21:15:17 gpu_executor.py:80] Maximum concurrency for 8192 tokens per request: 23.71x
INFO 01-21 21:15:19 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-21 21:15:19 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-21 21:15:34 model_runner.py:1527] Graph capturing finished in 15 secs, took 0.26 GiB
INFO 01-21 21:15:34 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 19.36 seconds
Evaluating tasks:   0%|          | 0/27 [00:00<?, ?it/s]=== Task: boolean_expressions - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:01<04:37,  1.11s/it, est. speed input: 30.54 toks/s, output: 0.90 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:01<00:00, 217.03it/s, est. speed input: 7379.72 toks/s, output: 217.05 toks/s]
Zero-shot Accuracy: 71.20%
Evaluation Time: 1.21 seconds

Evaluating tasks:   4%|â–Ž         | 1/27 [00:01<00:31,  1.21s/it]=== Task: causal_judgement - Zero-shot ===

Processed prompts:   0%|          | 0/187 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   1%|          | 1/187 [00:01<06:07,  1.98s/it, est. speed input: 208.98 toks/s, output: 0.51 toks/s][A
Processed prompts:  18%|â–ˆâ–Š        | 33/187 [00:02<00:11, 13.76it/s, est. speed input: 2767.31 toks/s, output: 11.12 toks/s][A
Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–      | 65/187 [00:03<00:05, 20.65it/s, est. speed input: 4113.10 toks/s, output: 16.40 toks/s][A
Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 97/187 [00:04<00:03, 24.36it/s, est. speed input: 4832.88 toks/s, output: 19.42 toks/s][A
Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 129/187 [00:05<00:01, 29.30it/s, est. speed input: 5609.30 toks/s, output: 22.45 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187/187 [00:05<00:00, 32.36it/s, est. speed input: 7957.68 toks/s, output: 32.36 toks/s]
Zero-shot Accuracy: 53.48%
Evaluation Time: 5.98 seconds

Evaluating tasks:   7%|â–‹         | 2/27 [00:07<01:40,  4.02s/it]=== Task: date_understanding - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:04<17:09,  4.13s/it, est. speed input: 26.85 toks/s, output: 0.73 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:04<00:00, 60.43it/s, est. speed input: 7161.05 toks/s, output: 181.30 toks/s]
Zero-shot Accuracy: 50.80%
Evaluation Time: 4.22 seconds

Evaluating tasks:  11%|â–ˆ         | 3/27 [00:11<01:38,  4.11s/it]=== Task: disambiguation_qa - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:03<14:01,  3.38s/it, est. speed input: 29.90 toks/s, output: 0.89 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:03<00:00, 73.94it/s, est. speed input: 7371.76 toks/s, output: 221.81 toks/s]
Zero-shot Accuracy: 29.20%
Evaluation Time: 3.46 seconds

Evaluating tasks:  15%|â–ˆâ–        | 4/27 [00:14<01:28,  3.86s/it]=== Task: dyck_languages - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:02<09:34,  2.31s/it, est. speed input: 26.44 toks/s, output: 1.30 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:02<00:00, 108.21it/s, est. speed input: 7131.48 toks/s, output: 324.65 toks/s]
Zero-shot Accuracy: 3.60%
Evaluation Time: 2.38 seconds

Evaluating tasks:  19%|â–ˆâ–Š        | 5/27 [00:17<01:13,  3.32s/it]=== Task: formal_fallacies - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:02<08:28,  2.04s/it, est. speed input: 85.69 toks/s, output: 0.49 toks/s][A
Processed prompts:  22%|â–ˆâ–ˆâ–       | 56/250 [00:03<00:08, 22.53it/s, est. speed input: 2682.41 toks/s, output: 18.15 toks/s][A
Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 113/250 [00:04<00:03, 34.64it/s, est. speed input: 3975.80 toks/s, output: 27.39 toks/s][A
Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 167/250 [00:04<00:01, 48.56it/s, est. speed input: 5222.63 toks/s, output: 35.67 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:04<00:00, 53.02it/s, est. speed input: 7788.36 toks/s, output: 53.02 toks/s]
Zero-shot Accuracy: 0.00%
Evaluation Time: 4.83 seconds

Evaluating tasks:  22%|â–ˆâ–ˆâ–       | 6/27 [00:22<01:20,  3.84s/it]=== Task: geometric_shapes - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:04<20:30,  4.94s/it, est. speed input: 24.69 toks/s, output: 0.61 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:04<00:00, 50.57it/s, est. speed input: 7495.59 toks/s, output: 151.71 toks/s]
Zero-shot Accuracy: 14.40%
Evaluation Time: 5.04 seconds

Evaluating tasks:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:27<01:24,  4.23s/it]=== Task: hyperbaton - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:01<08:09,  1.96s/it, est. speed input: 29.01 toks/s, output: 1.53 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:01<00:00, 127.02it/s, est. speed input: 7054.36 toks/s, output: 381.07 toks/s]
Zero-shot Accuracy: 58.00%
Evaluation Time: 2.03 seconds

Evaluating tasks:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:29<01:07,  3.53s/it]=== Task: logical_deduction_five_objects - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:05<24:00,  5.79s/it, est. speed input: 31.46 toks/s, output: 0.52 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:05<00:00, 43.19it/s, est. speed input: 7498.94 toks/s, output: 129.57 toks/s]
Zero-shot Accuracy: 41.60%
Evaluation Time: 5.91 seconds

Evaluating tasks:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:35<01:16,  4.27s/it]=== Task: logical_deduction_seven_objects - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:07<29:52,  7.20s/it, est. speed input: 29.44 toks/s, output: 0.42 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:07<00:00, 34.71it/s, est. speed input: 7558.13 toks/s, output: 104.12 toks/s]
Zero-shot Accuracy: 40.40%
Evaluation Time: 7.34 seconds

Evaluating tasks:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:42<01:28,  5.22s/it]=== Task: logical_deduction_three_objects - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:04<17:54,  4.31s/it, est. speed input: 30.37 toks/s, output: 0.70 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:04<00:00, 57.92it/s, est. speed input: 7428.36 toks/s, output: 173.75 toks/s]
Zero-shot Accuracy: 51.60%
Evaluation Time: 4.42 seconds

Evaluating tasks:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:46<01:19,  4.98s/it]=== Task: movie_recommendation - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:02<11:52,  2.86s/it, est. speed input: 28.32 toks/s, output: 1.05 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:02<00:00, 87.32it/s, est. speed input: 7221.94 toks/s, output: 261.97 toks/s]
Zero-shot Accuracy: 53.20%
Evaluation Time: 2.94 seconds

Evaluating tasks:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:49<01:05,  4.36s/it]=== Task: multistep_arithmetic_two - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:01<06:47,  1.64s/it, est. speed input: 27.46 toks/s, output: 1.83 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:01<00:00, 152.28it/s, est. speed input: 6780.99 toks/s, output: 456.86 toks/s]
Zero-shot Accuracy: 0.80%
Evaluation Time: 1.70 seconds

Evaluating tasks:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:51<00:49,  3.55s/it]=== Task: navigate - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:02<08:43,  2.10s/it, est. speed input: 39.96 toks/s, output: 0.48 toks/s][A
Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 96/250 [00:02<00:03, 45.32it/s, est. speed input: 2997.72 toks/s, output: 35.01 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:02<00:00, 89.88it/s, est. speed input: 7518.92 toks/s, output: 89.88 toks/s]
Zero-shot Accuracy: 44.00%
Evaluation Time: 2.86 seconds

Evaluating tasks:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:54<00:43,  3.34s/it]=== Task: object_counting - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:01<08:07,  1.96s/it, est. speed input: 28.59 toks/s, output: 1.02 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:01<00:00, 127.44it/s, est. speed input: 7207.17 toks/s, output: 254.89 toks/s]
Zero-shot Accuracy: 44.40%
Evaluation Time: 2.03 seconds

Evaluating tasks:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:56<00:35,  2.95s/it]=== Task: penguins_in_a_table - Zero-shot ===

Processed prompts:   0%|          | 0/146 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   1%|          | 1/146 [00:03<09:34,  3.96s/it, est. speed input: 41.67 toks/s, output: 0.76 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 146/146 [00:03<00:00, 36.86it/s, est. speed input: 7450.24 toks/s, output: 110.57 toks/s]
Zero-shot Accuracy: 47.26%
Evaluation Time: 4.04 seconds

Evaluating tasks:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [01:00<00:36,  3.27s/it]=== Task: reasoning_about_colored_objects - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:04<19:44,  4.76s/it, est. speed input: 31.75 toks/s, output: 0.63 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:04<00:00, 52.53it/s, est. speed input: 7370.03 toks/s, output: 157.58 toks/s]
Zero-shot Accuracy: 59.20%
Evaluation Time: 5.09 seconds

Evaluating tasks:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [01:05<00:38,  3.82s/it]=== Task: ruin_names - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:03<12:35,  3.03s/it, est. speed input: 28.35 toks/s, output: 0.99 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:03<00:00, 82.32it/s, est. speed input: 7212.96 toks/s, output: 246.96 toks/s]
Zero-shot Accuracy: 53.60%
Evaluation Time: 3.12 seconds

Evaluating tasks:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [01:08<00:32,  3.61s/it]=== Task: salient_translation_error_detection - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:09<38:09,  9.19s/it, est. speed input: 31.43 toks/s, output: 0.33 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:09<00:00, 27.18it/s, est. speed input: 7618.59 toks/s, output: 81.54 toks/s]
Zero-shot Accuracy: 42.00%
Evaluation Time: 9.37 seconds

Evaluating tasks:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [01:17<00:42,  5.34s/it]=== Task: snarks - Zero-shot ===

Processed prompts:   0%|          | 0/178 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   1%|          | 1/178 [00:01<05:30,  1.87s/it, est. speed input: 28.36 toks/s, output: 1.61 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 178/178 [00:01<00:00, 95.12it/s, est. speed input: 7140.77 toks/s, output: 285.35 toks/s]
Zero-shot Accuracy: 60.67%
Evaluation Time: 1.93 seconds

Evaluating tasks:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [01:19<00:30,  4.32s/it]=== Task: sports_understanding - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:01<06:26,  1.55s/it, est. speed input: 30.26 toks/s, output: 0.64 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:01<00:00, 156.80it/s, est. speed input: 7410.09 toks/s, output: 156.81 toks/s]
Zero-shot Accuracy: 76.00%
Evaluation Time: 1.66 seconds

Evaluating tasks:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [01:21<00:21,  3.52s/it]=== Task: temporal_sequences - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:06<27:08,  6.54s/it, est. speed input: 27.83 toks/s, output: 0.46 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:06<00:00, 38.22it/s, est. speed input: 7422.18 toks/s, output: 114.65 toks/s]
Zero-shot Accuracy: 93.20%
Evaluation Time: 6.67 seconds

Evaluating tasks:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [01:28<00:22,  4.46s/it]=== Task: tracking_shuffled_objects_five_objects - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:06<27:33,  6.64s/it, est. speed input: 28.76 toks/s, output: 0.45 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:06<00:00, 37.62it/s, est. speed input: 7510.00 toks/s, output: 112.88 toks/s]
Zero-shot Accuracy: 18.80%
Evaluation Time: 6.78 seconds

Evaluating tasks:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [01:35<00:20,  5.16s/it]=== Task: tracking_shuffled_objects_seven_objects - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:08<34:03,  8.21s/it, est. speed input: 29.36 toks/s, output: 0.37 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:08<00:00, 30.45it/s, est. speed input: 7606.00 toks/s, output: 91.34 toks/s]
Zero-shot Accuracy: 16.80%
Evaluation Time: 8.36 seconds

Evaluating tasks:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [01:43<00:18,  6.12s/it]=== Task: tracking_shuffled_objects_three_objects - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:05<21:58,  5.29s/it, est. speed input: 30.60 toks/s, output: 0.57 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:05<00:00, 47.19it/s, est. speed input: 7380.25 toks/s, output: 141.58 toks/s]
Zero-shot Accuracy: 30.00%
Evaluation Time: 5.41 seconds

Evaluating tasks:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [01:48<00:11,  5.91s/it]=== Task: web_of_lies - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:02<08:49,  2.13s/it, est. speed input: 32.45 toks/s, output: 0.47 toks/s][A
Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 115/250 [00:02<00:01, 67.63it/s, est. speed input: 3520.92 toks/s, output: 49.28 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:02<00:00, 105.56it/s, est. speed input: 7532.62 toks/s, output: 105.56 toks/s]
Zero-shot Accuracy: 0.00%
Evaluation Time: 2.44 seconds

Evaluating tasks:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [01:51<00:04,  4.87s/it]=== Task: word_sorting - Zero-shot ===

Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 1/250 [00:02<08:26,  2.03s/it, est. speed input: 15.25 toks/s, output: 2.46 toks/s][A
Processed prompts:   2%|â–         | 6/250 [00:02<01:06,  3.68it/s, est. speed input: 88.54 toks/s, output: 16.23 toks/s][A
Processed prompts:   7%|â–‹         | 18/250 [00:02<00:17, 13.25it/s, est. speed input: 260.85 toks/s, output: 56.12 toks/s][A
Processed prompts:  13%|â–ˆâ–Ž        | 33/250 [00:02<00:07, 27.16it/s, est. speed input: 469.10 toks/s, output: 112.07 toks/s][A
Processed prompts:  17%|â–ˆâ–‹        | 42/250 [00:02<00:06, 34.52it/s, est. speed input: 584.00 toks/s, output: 150.57 toks/s][A
Processed prompts:  22%|â–ˆâ–ˆâ–       | 56/250 [00:02<00:03, 49.38it/s, est. speed input: 768.84 toks/s, output: 216.41 toks/s][A
Processed prompts:  26%|â–ˆâ–ˆâ–‹       | 66/250 [00:02<00:03, 58.07it/s, est. speed input: 892.59 toks/s, output: 265.36 toks/s][A
Processed prompts:  31%|â–ˆâ–ˆâ–ˆ       | 77/250 [00:02<00:02, 62.42it/s, est. speed input: 1015.91 toks/s, output: 321.76 toks/s][A
Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 96/250 [00:03<00:01, 81.18it/s, est. speed input: 1264.26 toks/s, output: 440.01 toks/s][A
Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 107/250 [00:03<00:01, 80.31it/s, est. speed input: 1382.45 toks/s, output: 504.66 toks/s][A
Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 123/250 [00:03<00:01, 90.00it/s, est. speed input: 1582.03 toks/s, output: 614.86 toks/s][A
Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 134/250 [00:03<00:01, 82.95it/s, est. speed input: 1689.58 toks/s, output: 684.07 toks/s][A
Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 144/250 [00:04<00:02, 38.58it/s, est. speed input: 1569.56 toks/s, output: 673.12 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:04<00:00, 60.34it/s, est. speed input: 3094.36 toks/s, output: 1952.04 toks/s]
Zero-shot Accuracy: 50.00%
Evaluation Time: 4.22 seconds

Evaluating tasks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [01:55<00:00,  4.68s/it]Evaluating tasks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [01:55<00:00,  4.28s/it]

Results Summary:

{
  "task": "boolean_expressions",
  "ZSL_1_accuracy": 71.2,
  "eval_time": 1.2088,
  "examples": [
    {
      "question": "True or True or not False or False is",
      "prediction": "true",
      "true_answer": "true"
    },
    {
      "question": "True or ( True or not True ) is",
      "prediction": "true",
      "true_answer": "true"
    },
    {
      "question": "not ( not not True ) or False is",
      "prediction": "true",
      "true_answer": "false"
    },
    {
      "question": "not ( True or True ) and True is",
      "prediction": "false",
      "true_answer": "false"
    },
    {
      "question": "not False or ( True or True ) is",
      "prediction": "true",
      "true_answer": "true"
    }
  ]
}
{
  "task": "causal_judgement",
  "ZSL_1_accuracy": 53.4759,
  "eval_time": 5.9816,
  "examples": [
    {
      "question": "How would a typical person answer each of the following questions about causation?\nLong ago, when John was only 17 years old, he got a job working for a large manufacturing company. He started out working on an assembly line for minimum wage, but after a few years at the company, he was given a choice between two line manager positions. He could stay in the woodwork division, which is where he was currently working. Or he could move to the plastics division. John was unsure what to do because he liked working in the woodwork division, but he also thought it might be worth trying something different. He finally decided to switch to the plastics division and try something new. For the last 30 years, John has worked as a production line supervisor in the plastics division. After the first year there, the plastics division was moved to a different building with more space. Unfortunately, through the many years he worked there, John was exposed to asbestos, a highly carcinogenic substance. Most of the plastics division was quite safe, but the small part in which John worked was exposed to asbestos fibers. And now, although John has never smoked a cigarette in his life and otherwise lives a healthy lifestyle, he has a highly progressed and incurable case of lung cancer at the age of 50. John had seen three cancer specialists, all of whom confirmed the worst: that, except for pain, John's cancer was untreatable and he was absolutely certain to die from it very soon (the doctors estimated no more than 2 months). Yesterday, while John was in the hospital for a routine medical appointment, a new nurse accidentally administered the wrong medication to him. John was allergic to the drug and he immediately went into shock and experienced cardiac arrest (a heart attack). Doctors attempted to resuscitate him but he died minutes after the medication was administered. Did misadministration of medication cause John's premature death?\nOptions:\n- Yes\n- No",
      "prediction": "yes",
      "true_answer": "yes"
    },
    {
      "question": "How would a typical person answer each of the following questions about causation?\nA sniper has been ordered to kill an enemy commander. So, after getting himself into position, he finally has the enemy commander in his sights. Before he pulls the trigger, however, the sniper realizes that the gunfire will definitely alert the other enemy soldiers to his presence. But the sniper doesn't care at all about that -- he just wants to shoot his target. So, he pulls the trigger -- thereby shooting and killing the commander. And, as he expected, the enemy soldiers are alerted to his presence. Did the sniper intentionally alert the enemies to his presence?\nOptions:\n- Yes\n- No",
      "prediction": "no",
      "true_answer": "yes"
    },
    {
      "question": "How would a typical person answer each of the following questions about causation?\nNed has a new motorboat. When Ned turns the key, the motorboat starts if either the gear is in neutral or the motor is in the lock position. Today, the gear is in neutral, and the motor is not in the lock position. Ned checks the motor to see if it is in the lock position. He changes its position, and he puts it in the lock position. Because the motorboat would start if either the gear is in neutral or the motor is in the lock position, the motorboat starts when Ned turns the key. Did the motorboat start because Ned changed the position of the motor?\nOptions:\n- Yes\n- No",
      "prediction": "no",
      "true_answer": "no"
    },
    {
      "question": "How would a typical person answer each of the following questions about causation?\nGeorge and his sister Lena reunite at their parents' house for Thanksgiving. Whereas George just got into medical school, Lena is unhappy in her marriage and recently lost her job. Over the course of the day, George and Lena get into a number of heated arguments. Later in the afternoon they play a game of darts. They split the first two games, and the third game is close until the end. Who will win comes down to George's last shot. If he hits a high point region, he wins; if he hits a low point region, Lena wins. George doesn't care that Lena is having a difficult time; he really wants to beat her. He aims the dart at the high point region. He sets up his shot and the dart lands in the high point region. George triumphs in his victory while Lena is sad. Did George hit the high point region intentionally?\nOptions:\n- Yes\n- No",
      "prediction": "yes",
      "true_answer": "yes"
    },
    {
      "question": "How would a typical person answer each of the following questions about causation?\nAlex will only win the game if the total of his dice roll is greater than 11 AND the coin comes up heads. It is very unlikely that he will roll higher than 11, but the coin has equal odds of coming up heads or tails. Alex flips the coin and rolls his dice at exactly the same time. The coin comes up heads, and he rolls a 12, so amazingly, he rolled greater than 11. Alex wins the game. Did Alex win because of the coin flip?\nOptions:\n- Yes\n- No",
      "prediction": "no",
      "true_answer": "no"
    }
  ]
}
{
  "task": "date_understanding",
  "ZSL_1_accuracy": 50.8,
  "eval_time": 4.2223,
  "examples": [
    {
      "question": "Today, 8/3/1997, is a day that we will never forget. What is the date one year ago from today in MM/DD/YYYY?\nOptions:\n(A) 10/07/1996\n(B) 08/03/2035\n(C) 05/03/1997\n(D) 08/03/1996\n(E) 08/04/1996",
      "prediction": "(d)",
      "true_answer": "(d)"
    },
    {
      "question": "Jane was born on the last day of Feburary in 2001. Today is her 16-year-old birthday. What is the date 10 days ago in MM/DD/YYYY?\nOptions:\n(A) 02/28/2017\n(B) 02/18/2017\n(C) 03/18/2017\n(D) 02/13/2017\n(E) 02/25/2017\n(F) 03/04/2017",
      "prediction": "(b)",
      "true_answer": "(b)"
    },
    {
      "question": "Jane and John married on Jan 2, 1958. It is their 5-year anniversary today. What is the date one week ago from today in MM/DD/YYYY?\nOptions:\n(A) 01/09/1961\n(B) 01/02/1961\n(C) 10/01/1960\n(D) 12/26/1960\n(E) 07/26/1960\n(F) 12/26/1936",
      "prediction": "(d)",
      "true_answer": "(d)"
    },
    {
      "question": "Jane was born on the last day of Feburary in 2001. Today is her 16-year-old birthday. What is the date one week ago from today in MM/DD/YYYY?\nOptions:\n(A) 03/03/2017\n(B) 04/27/2017\n(C) 03/21/2017\n(D) 02/21/1961\n(E) 02/21/2017",
      "prediction": "(e)",
      "true_answer": "(e)"
    },
    {
      "question": "Jane visits the bookstore on the 16th of each month starting from the October of 2009. It is her 5th visit to the bookstore today. What is the date one week from today in MM/DD/YYYY?\nOptions:\n(A) 02/22/2010\n(B) 02/23/2010\n(C) 03/01/2010\n(D) 01/26/2010\n(E) 02/19/2010\n(F) 02/24/2010",
      "prediction": "(c)",
      "true_answer": "(b)"
    }
  ]
}
{
  "task": "disambiguation_qa",
  "ZSL_1_accuracy": 29.2,
  "eval_time": 3.4641,
  "examples": [
    {
      "question": "In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\nSentence: The investigator wanted to interview the witness in person, but he was too late.\nOptions:\n(A) The investigator was too late\n(B) The witness was too late\n(C) Ambiguous",
      "prediction": "(c)",
      "true_answer": "(c)"
    },
    {
      "question": "In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\nSentence: The nurse was running from the laborer because she smelled awful.\nOptions:\n(A) The nurse smelled awful\n(B) The laborer smelled awful\n(C) Ambiguous",
      "prediction": "(c)",
      "true_answer": "(c)"
    },
    {
      "question": "In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\nSentence: The writer tried to fool the salesperson and told her a fake story.\nOptions:\n(A) Told the writer a fake story\n(B) Told the salesperson a fake story\n(C) Ambiguous",
      "prediction": "(c)",
      "true_answer": "(b)"
    },
    {
      "question": "In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\nSentence: The physician told the baker that they had cancer.\nOptions:\n(A) The physician had cancer\n(B) The baker had cancer\n(C) Ambiguous",
      "prediction": "(c)",
      "true_answer": "(b)"
    },
    {
      "question": "In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\nSentence: The secretary came to the analyst's office and helped her to book a flight.\nOptions:\n(A) Helped the secretary book a flight\n(B) Helped the analyst book a flight\n(C) Ambiguous",
      "prediction": "(c)",
      "true_answer": "(b)"
    }
  ]
}
{
  "task": "dyck_languages",
  "ZSL_1_accuracy": 3.6,
  "eval_time": 2.3765,
  "examples": [
    {
      "question": "Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: < { } [ [ ( ( < { < { [ ] } > } > ) )",
      "prediction": "< { }",
      "true_answer": "] ] >"
    },
    {
      "question": "Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: ( { ( )",
      "prediction": "( { (",
      "true_answer": "} )"
    },
    {
      "question": "Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: < [ ( { { ( ( ) ) } } ) [ ( [ { } ] ) ] < { { < < < > [ < [ < ( [ ( { ( ( < < < < > > > { ( { { < ( ) > ( ) } } ) } > { } ) ) } ) ] ) > ] > ] > < { } > > } ( ) < { ( ) } > } > ] [ < ( ) > ]",
      "prediction": "( ) (",
      "true_answer": ">"
    },
    {
      "question": "Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: [ < [ ]",
      "prediction": "[ [ ]",
      "true_answer": "> ]"
    },
    {
      "question": "Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: { [ < [ ] > ] } { ( ) [ < > ] [ [ [ ] ]",
      "prediction": "{ [ <",
      "true_answer": "] }"
    }
  ]
}
{
  "task": "formal_fallacies",
  "ZSL_1_accuracy": 0.0,
  "eval_time": 4.8284,
  "examples": [
    {
      "question": "\"Here comes a perfectly valid argument: To begin with, whatever is both an ingredient of Immense Mascara and an ingredient of I Adore You is an ingredient of Crawford's Wine or an ingredient of Bioglitz Color 9N.moreover, being an ingredient of lip2cheek is necessary for being an ingredient of Bioglitz Color 9N. Finally, everything that is an ingredient of Crawford's Wine is an ingredient of lip2cheek, too. Therefore, whatever is an ingredient of Immense Mascara and an ingredient of I Adore You is also an ingredient of lip2cheek.\"\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\nOptions:\n- valid \n- invalid",
      "prediction": "the",
      "true_answer": "valid"
    },
    {
      "question": "\"Here comes a perfectly valid argument: First, Andres is a member of Paris Saint-Germain. Second, it is false that Andres is a follower of FC Mariupol. Third, whoever is a follower of FC Mariupol and a member of Paris Saint-Germain is also a devotee of FC Arouca.therefore, Andres is not a devotee of FC Arouca.\"\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\nOptions:\n- valid \n- invalid",
      "prediction": "-",
      "true_answer": "invalid"
    },
    {
      "question": "\"Here comes a perfectly valid argument: To begin with, it is false that Retinyl acetate is an ingredient of Spider Hero Tattoo. Moreover, every ingredient of Peach Whip is an ingredient of Spider Hero Tattoo. It follows that it is false that Retinyl acetate is an ingredient of Peach Whip.\"\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\nOptions:\n- valid \n- invalid",
      "prediction": "-",
      "true_answer": "valid"
    },
    {
      "question": "\"It is not always easy to see who is related to whom -- and in which ways. The following argument pertains to this question: It is not the case that Manuel is a half-brother of Chester. Whoever is an ancestor of Jeremy is not a half-brother of Chester. Hence, it is false that Manuel is an ancestor of Jeremy.\"\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\nOptions:\n- valid \n- invalid",
      "prediction": "the",
      "true_answer": "invalid"
    },
    {
      "question": "\"Here comes a perfectly valid argument: First of all, everyone who is a loyal buyer of Herbal Essences shampoo is also an infrequent user of Caswell-Massey soap and a loyal buyer of Matrix shampoo. Next, Christian is not an infrequent user of Caswell-Massey soap or not a loyal buyer of Matrix shampoo. So, necessarily, Christian is not a loyal buyer of Herbal Essences shampoo.\"\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\nOptions:\n- valid \n- invalid",
      "prediction": "-",
      "true_answer": "valid"
    }
  ]
}
{
  "task": "geometric_shapes",
  "ZSL_1_accuracy": 14.4,
  "eval_time": 5.0397,
  "examples": [
    {
      "question": "This SVG path element <path d=\"M 62.00,77.00 L 17.00,31.00 L 96.00,39.00 L 62.00,77.00\"/> draws a\nOptions:\n(A) circle\n(B) heptagon\n(C) hexagon\n(D) kite\n(E) line\n(F) octagon\n(G) pentagon\n(H) rectangle\n(I) sector\n(J) triangle",
      "prediction": "(e)",
      "true_answer": "(j)"
    },
    {
      "question": "This SVG path element <path d=\"M 99.00,97.00 L 10.00,97.00 L 10.00,49.00 L 99.00,49.00 L 99.00,97.00\"/> draws a\nOptions:\n(A) circle\n(B) heptagon\n(C) hexagon\n(D) kite\n(E) line\n(F) octagon\n(G) pentagon\n(H) rectangle\n(I) sector\n(J) triangle\n(K) trapezoid",
      "prediction": "(h)",
      "true_answer": "(k)"
    },
    {
      "question": "This SVG path element <path d=\"M 25.00,38.00 L 89.00,58.00\"/> draws a\nOptions:\n(A) circle\n(B) heptagon\n(C) hexagon\n(D) kite\n(E) line\n(F) octagon\n(G) pentagon\n(H) rectangle\n(I) sector\n(J) triangle",
      "prediction": "(e)",
      "true_answer": "(e)"
    },
    {
      "question": "This SVG path element <path d=\"M 51.00,18.00 L 5.00,62.00\"/> draws a\nOptions:\n(A) circle\n(B) heptagon\n(C) hexagon\n(D) kite\n(E) line\n(F) octagon\n(G) pentagon\n(H) rectangle\n(I) sector\n(J) triangle",
      "prediction": "(e)",
      "true_answer": "(e)"
    },
    {
      "question": "This SVG path element <path d=\"M 33.22,28.05 L 35.20,42.02 L 12.90,21.04 M 12.90,21.04 L 34.12,57.25 M 34.12,57.25 L 47.79,34.91 M 47.79,34.91 L 33.22,28.05\"/> draws a\nOptions:\n(A) circle\n(B) heptagon\n(C) hexagon\n(D) kite\n(E) line\n(F) octagon\n(G) pentagon\n(H) rectangle\n(I) sector\n(J) triangle",
      "prediction": "(h)",
      "true_answer": "(g)"
    }
  ]
}
{
  "task": "hyperbaton",
  "ZSL_1_accuracy": 58.0,
  "eval_time": 2.0312,
  "examples": [
    {
      "question": "Which sentence has the correct adjective order:\nOptions:\n(A) walking black medium-size rectangular wonderful car\n(B) wonderful medium-size rectangular black walking car",
      "prediction": "(a)",
      "true_answer": "(b)"
    },
    {
      "question": "Which sentence has the correct adjective order:\nOptions:\n(A) big square ship\n(B) square big ship",
      "prediction": "(a)",
      "true_answer": "(a)"
    },
    {
      "question": "Which sentence has the correct adjective order:\nOptions:\n(A) repulsive rectangular black huge lead match\n(B) repulsive huge rectangular black lead match",
      "prediction": "(a)",
      "true_answer": "(b)"
    },
    {
      "question": "Which sentence has the correct adjective order:\nOptions:\n(A) nice little pyramidal red Nigerian drinking car\n(B) little nice Nigerian red drinking pyramidal car",
      "prediction": "(a)",
      "true_answer": "(a)"
    },
    {
      "question": "Which sentence has the correct adjective order:\nOptions:\n(A) massive cardboard good Thai sweater\n(B) good massive Thai cardboard sweater",
      "prediction": "(a)",
      "true_answer": "(b)"
    }
  ]
}
{
  "task": "logical_deduction_five_objects",
  "ZSL_1_accuracy": 41.6,
  "eval_time": 5.9061,
  "examples": [
    {
      "question": "The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. In an antique car show, there are five vehicles: a tractor, a station wagon, a bus, a motorcyle, and a minivan. The minivan is older than the motorcyle. The bus is the newest. The tractor is the third-newest. The station wagon is the second-oldest.\nOptions:\n(A) The tractor is the third-newest\n(B) The station wagon is the third-newest\n(C) The bus is the third-newest\n(D) The motorcyle is the third-newest\n(E) The minivan is the third-newest",
      "prediction": "(a)",
      "true_answer": "(a)"
    },
    {
      "question": "The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. In an antique car show, there are five vehicles: a minivan, a hatchback, a bus, a convertible, and a motorcyle. The hatchback is newer than the convertible. The bus is newer than the hatchback. The bus is older than the motorcyle. The minivan is the newest.\nOptions:\n(A) The minivan is the oldest\n(B) The hatchback is the oldest\n(C) The bus is the oldest\n(D) The convertible is the oldest\n(E) The motorcyle is the oldest",
      "prediction": "(e)",
      "true_answer": "(d)"
    },
    {
      "question": "The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. A fruit stand sells five fruits: watermelons, apples, kiwis, cantaloupes, and mangoes. The cantaloupes are the second-cheapest. The mangoes are more expensive than the watermelons. The kiwis are less expensive than the apples. The kiwis are more expensive than the mangoes.\nOptions:\n(A) The watermelons are the most expensive\n(B) The apples are the most expensive\n(C) The kiwis are the most expensive\n(D) The cantaloupes are the most expensive\n(E) The mangoes are the most expensive",
      "prediction": "(e)",
      "true_answer": "(b)"
    },
    {
      "question": "The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were five golfers: Eve, Rob, Ana, Eli, and Mya. Eli finished below Mya. Eve finished first. Ana finished below Eli. Rob finished second.\nOptions:\n(A) Eve finished second-to-last\n(B) Rob finished second-to-last\n(C) Ana finished second-to-last\n(D) Eli finished second-to-last\n(E) Mya finished second-to-last",
      "prediction": "(e)",
      "true_answer": "(d)"
    },
    {
      "question": "The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were five golfers: Joe, Mya, Eve, Amy, and Ana. Eve finished below Joe. Ana finished below Amy. Eve finished above Amy. Mya finished above Joe.\nOptions:\n(A) Joe finished third\n(B) Mya finished third\n(C) Eve finished third\n(D) Amy finished third\n(E) Ana finished third",
      "prediction": "(c)",
      "true_answer": "(c)"
    }
  ]
}
{
  "task": "logical_deduction_seven_objects",
  "ZSL_1_accuracy": 40.4,
  "eval_time": 7.3405,
  "examples": [
    {
      "question": "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. In an antique car show, there are seven vehicles: a motorcyle, a sedan, a station wagon, a tractor, a minivan, a bus, and a truck. The motorcyle is older than the minivan. The minivan is older than the tractor. The bus is newer than the truck. The station wagon is the third-newest. The sedan is older than the truck. The tractor is older than the sedan.\nOptions:\n(A) The motorcyle is the oldest\n(B) The sedan is the oldest\n(C) The station wagon is the oldest\n(D) The tractor is the oldest\n(E) The minivan is the oldest\n(F) The bus is the oldest\n(G) The truck is the oldest",
      "prediction": "(c)",
      "true_answer": "(a)"
    },
    {
      "question": "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were seven golfers: Ana, Eve, Ada, Dan, Rob, Amy, and Joe. Dan finished third. Ana finished above Ada. Amy finished last. Dan finished below Rob. Eve finished below Ada. Rob finished below Joe.\nOptions:\n(A) Ana finished last\n(B) Eve finished last\n(C) Ada finished last\n(D) Dan finished last\n(E) Rob finished last\n(F) Amy finished last\n(G) Joe finished last",
      "prediction": "(f)",
      "true_answer": "(f)"
    },
    {
      "question": "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. A fruit stand sells seven fruits: plums, kiwis, cantaloupes, pears, watermelons, apples, and loquats. The watermelons are more expensive than the cantaloupes. The apples are less expensive than the cantaloupes. The watermelons are the second-most expensive. The loquats are less expensive than the kiwis. The apples are more expensive than the loquats. The loquats are the third-cheapest. The plums are the cheapest.\nOptions:\n(A) The plums are the second-cheapest\n(B) The kiwis are the second-cheapest\n(C) The cantaloupes are the second-cheapest\n(D) The pears are the second-cheapest\n(E) The watermelons are the second-cheapest\n(F) The apples are the second-cheapest\n(G) The loquats are the second-cheapest",
      "prediction": "(e)",
      "true_answer": "(d)"
    },
    {
      "question": "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were seven golfers: Eli, Ada, Amy, Ana, Eve, Mel, and Dan. Ada finished above Mel. Dan finished above Ada. Amy finished last. Ana finished third-to-last. Dan finished below Eli. Eve finished third.\nOptions:\n(A) Eli finished third-to-last\n(B) Ada finished third-to-last\n(C) Amy finished third-to-last\n(D) Ana finished third-to-last\n(E) Eve finished third-to-last\n(F) Mel finished third-to-last\n(G) Dan finished third-to-last",
      "prediction": "(d)",
      "true_answer": "(d)"
    },
    {
      "question": "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were seven golfers: Joe, Dan, Ada, Amy, Rob, Mya, and Mel. Ada finished below Amy. Joe finished below Dan. Dan finished below Ada. Mel finished third-to-last. Amy finished third. Rob finished below Mya.\nOptions:\n(A) Joe finished third-to-last\n(B) Dan finished third-to-last\n(C) Ada finished third-to-last\n(D) Amy finished third-to-last\n(E) Rob finished third-to-last\n(F) Mya finished third-to-last\n(G) Mel finished third-to-last",
      "prediction": "(g)",
      "true_answer": "(g)"
    }
  ]
}
{
  "task": "logical_deduction_three_objects",
  "ZSL_1_accuracy": 51.6,
  "eval_time": 4.4161,
  "examples": [
    {
      "question": "The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. A fruit stand sells three fruits: oranges, mangoes, and plums. The plums are less expensive than the mangoes. The oranges are less expensive than the plums.\nOptions:\n(A) The oranges are the second-most expensive\n(B) The mangoes are the second-most expensive\n(C) The plums are the second-most expensive",
      "prediction": "(c)",
      "true_answer": "(c)"
    },
    {
      "question": "The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are three birds: a robin, a blue jay, and a crow. The robin is to the right of the crow. The robin is to the left of the blue jay.\nOptions:\n(A) The robin is the second from the left\n(B) The blue jay is the second from the left\n(C) The crow is the second from the left",
      "prediction": "(c)",
      "true_answer": "(a)"
    },
    {
      "question": "The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were three golfers: Dan, Mel, and Amy. Dan finished above Amy. Amy finished above Mel.\nOptions:\n(A) Dan finished last\n(B) Mel finished last\n(C) Amy finished last",
      "prediction": "(c)",
      "true_answer": "(b)"
    },
    {
      "question": "The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are three birds: an owl, a crow, and a cardinal. The crow is to the left of the owl. The owl is the second from the left.\nOptions:\n(A) The owl is the leftmost\n(B) The crow is the leftmost\n(C) The cardinal is the leftmost",
      "prediction": "(b)",
      "true_answer": "(b)"
    },
    {
      "question": "The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a white book, a black book, and a purple book. The black book is to the left of the purple book. The purple book is to the left of the white book.\nOptions:\n(A) The white book is the second from the left\n(B) The black book is the second from the left\n(C) The purple book is the second from the left",
      "prediction": "(c)",
      "true_answer": "(c)"
    }
  ]
}
{
  "task": "movie_recommendation",
  "ZSL_1_accuracy": 53.2,
  "eval_time": 2.9437,
  "examples": [
    {
      "question": "Find a movie similar to Stargate, The Fugitive, Pulp Fiction, Dances with Wolves:\nOptions:\n(A) Forrest Gump\n(B) Kevin Smith Too Fat For 40\n(C) Scorched\n(D) Stand and Deliver",
      "prediction": "(a)",
      "true_answer": "(a)"
    },
    {
      "question": "Find a movie similar to Dances with Wolves, Stargate, Pulp Fiction, The Shawshank Redemption:\nOptions:\n(A) The Towering Inferno\n(B) The Collector\n(C) Diabolique\n(D) The Fugitive",
      "prediction": "(d)",
      "true_answer": "(d)"
    },
    {
      "question": "Find a movie similar to Dances with Wolves, Crimson Tide, Braveheart, The Fugitive:\nOptions:\n(A) Rachel Getting Married\n(B) Howard the Duck\n(C) The Shawshank Redemption\n(D) The Post",
      "prediction": "(c)",
      "true_answer": "(c)"
    },
    {
      "question": "Find a movie similar to Terminator 2 Judgment Day, The Lion King, Batman, Aladdin:\nOptions:\n(A) Killing Me Softly\n(B) A King in New York\n(C) The Proposition\n(D) Pretty Woman",
      "prediction": "(a)",
      "true_answer": "(d)"
    },
    {
      "question": "Find a movie similar to The Terminator, Toy Story, The Sixth Sense, Star Wars Episode VI - Return of the Jedi:\nOptions:\n(A) Indecent Proposal\n(B) Star Wars Episode V - The Empire Strikes Back\n(C) Gray's Anatomy\n(D) The Girl Next Door",
      "prediction": "(b)",
      "true_answer": "(b)"
    }
  ]
}
{
  "task": "multistep_arithmetic_two",
  "ZSL_1_accuracy": 0.8,
  "eval_time": 1.6962,
  "examples": [
    {
      "question": "((-2 - 5 * -5 - 6) + (-5 * -4 + 2 + -2)) =",
      "prediction": "(-",
      "true_answer": "37"
    },
    {
      "question": "((-9 - -7 + 2 + 2) + (-7 + -9 - -1 - 0)) =",
      "prediction": "-7",
      "true_answer": "-13"
    },
    {
      "question": "((7 * 2 + 4 - -6) * (6 - 7 - 7 * -2)) =",
      "prediction": "(7",
      "true_answer": "312"
    },
    {
      "question": "((-1 - 9 * 6 + 3) * (0 - 8 - -4 - -4)) =",
      "prediction": "((-1",
      "true_answer": "0"
    },
    {
      "question": "((3 + 3 - -4 - 2) + (-2 - -7 * -2 * 1)) =",
      "prediction": "3",
      "true_answer": "-8"
    }
  ]
}
{
  "task": "navigate",
  "ZSL_1_accuracy": 44.0,
  "eval_time": 2.8596,
  "examples": [
    {
      "question": "If you follow these instructions, do you return to the starting point? Always face forward. Take 7 steps forward. Take 7 steps backward. Take 10 steps backward. Take 10 steps forward.\nOptions:\n- Yes\n- No",
      "prediction": "the",
      "true_answer": "yes"
    },
    {
      "question": "If you follow these instructions, do you return to the starting point? Take 3 steps. Take 6 steps. Take 6 steps. Turn left. Take 8 steps. Take 1 step. Take 3 steps. Take 7 steps. Take 8 steps.\nOptions:\n- Yes\n- No",
      "prediction": "no",
      "true_answer": "no"
    },
    {
      "question": "If you follow these instructions, do you return to the starting point? Always face forward. Take 4 steps forward. Take 5 steps left. Take 6 steps forward. Take 8 steps backward. Take 5 steps backward. Take 1 step backward. Take 1 step backward.\nOptions:\n- Yes\n- No",
      "prediction": "no",
      "true_answer": "no"
    },
    {
      "question": "If you follow these instructions, do you return to the starting point? Take 6 steps. Take 5 steps. Take 6 steps.\nOptions:\n- Yes\n- No",
      "prediction": "no",
      "true_answer": "no"
    },
    {
      "question": "If you follow these instructions, do you return to the starting point? Always face forward. Take 4 steps right. Take 5 steps right. Take 9 steps left.\nOptions:\n- Yes\n- No",
      "prediction": "no",
      "true_answer": "yes"
    }
  ]
}
{
  "task": "object_counting",
  "ZSL_1_accuracy": 44.4,
  "eval_time": 2.0253,
  "examples": [
    {
      "question": "I have a fridge, an oven, a car, a toaster, a microwave, a table, and a bed. How many objects do I have?",
      "prediction": "7",
      "true_answer": "7"
    },
    {
      "question": "I have a peach, three cats, and a raspberry. How many fruits do I have?",
      "prediction": "1",
      "true_answer": "2"
    },
    {
      "question": "I have two violins, a drum, a flute, a trumpet, and a clarinet. How many musical instruments do I have?",
      "prediction": "6",
      "true_answer": "6"
    },
    {
      "question": "I have a goat, a fish, a piano, a violin, a trombone, a chicken, four clarinets, a drum, and an accordion. How many musical instruments do I have?",
      "prediction": "8",
      "true_answer": "9"
    },
    {
      "question": "I have a head of broccoli, a carrot, a potato, a yam, and a cauliflower. How many vegetables do I have?",
      "prediction": "5",
      "true_answer": "5"
    }
  ]
}
{
  "task": "penguins_in_a_table",
  "ZSL_1_accuracy": 47.2603,
  "eval_time": 4.0358,
  "examples": [
    {
      "question": "Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  Which is the heaviest penguin?\nOptions:\n(A) Louis\n(B) Bernard\n(C) Vincent\n(D) Gwen\n(E) James",
      "prediction": "(d)",
      "true_answer": "(d)"
    },
    {
      "question": "Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  We now add a penguin to the table:\nJames, 12, 90, 12\nWhat is the average height of the penguins?\nOptions:\n(A) 60\n(B) 65\n(C) 70\n(D) 75\n(E) 80",
      "prediction": "(c)",
      "true_answer": "(c)"
    },
    {
      "question": "Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  We now add a penguin to the table:\nJames, 12, 90, 12\nWe then delete the penguin named Bernard from the table.\nWhat is the cumulated weight of the penguins?\nOptions:\n(A) 37\n(B) 49\n(C) 50\n(D) 62\n(E) 63",
      "prediction": "(d)",
      "true_answer": "(b)"
    },
    {
      "question": "Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  Which is the shortest penguin?\nOptions:\n(A) Louis\n(B) Bernard\n(C) Vincent\n(D) Gwen\n(E) James",
      "prediction": "(a)",
      "true_answer": "(a)"
    },
    {
      "question": "Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  We now add a penguin to the table:\nJames, 12, 90, 12\nAnd here is a similar table, but listing giraffes:\nname, age, height (cm), weight (kg)\nJody, 5, 430, 620\nGladys, 10, 420, 590\nMarian, 2, 310, 410\nDonna, 9, 440, 650\nHow many species are listed in the tables?\nOptions:\n(A) 1\n(B) 2\n(C) 3\n(D) 4\n(E) 5",
      "prediction": "(b)",
      "true_answer": "(b)"
    }
  ]
}
{
  "task": "reasoning_about_colored_objects",
  "ZSL_1_accuracy": 59.2,
  "eval_time": 5.0924,
  "examples": [
    {
      "question": "On the desk, you see a bunch of items arranged in a row: a purple puzzle, a black envelope, and a blue textbook. What is the color of the item furthest from the puzzle?\nOptions:\n(A) red\n(B) orange\n(C) yellow\n(D) green\n(E) blue\n(F) brown\n(G) magenta\n(H) fuchsia\n(I) mauve\n(J) teal\n(K) turquoise\n(L) burgundy\n(M) silver\n(N) gold\n(O) black\n(P) grey\n(Q) purple\n(R) pink",
      "prediction": "(o)",
      "true_answer": "(e)"
    },
    {
      "question": "On the nightstand, you see one green necklace, two green textbooks, and two blue textbooks. If I remove all the textbooks from the nightstand, how many green items remain on it?\nOptions:\n(A) zero\n(B) one\n(C) two\n(D) three\n(E) four\n(F) five\n(G) six\n(H) seven\n(I) eight\n(J) nine\n(K) ten\n(L) eleven\n(M) twelve\n(N) thirteen\n(O) fourteen\n(P) fifteen\n(Q) sixteen",
      "prediction": "(b)",
      "true_answer": "(b)"
    },
    {
      "question": "On the table, you see the following objects arranged in a row: a burgundy plate, a turquoise keychain, and a gold puzzle. What is the color of the object directly to the left of the gold object?\nOptions:\n(A) red\n(B) orange\n(C) yellow\n(D) green\n(E) blue\n(F) brown\n(G) magenta\n(H) fuchsia\n(I) mauve\n(J) teal\n(K) turquoise\n(L) burgundy\n(M) silver\n(N) gold\n(O) black\n(P) grey\n(Q) purple\n(R) pink",
      "prediction": "(l)",
      "true_answer": "(k)"
    },
    {
      "question": "On the floor, you see a purple bracelet and a red scrunchiephone charger. How many things are neither red nor purple?\nOptions:\n(A) zero\n(B) one\n(C) two\n(D) three\n(E) four\n(F) five\n(G) six",
      "prediction": "(a)",
      "true_answer": "(a)"
    },
    {
      "question": "On the floor, you see a silver mug, an orange textbook, a brown pair of sunglasses, a turquoise paperclip, and a yellow notebook. What color is the mug?\nOptions:\n(A) red\n(B) orange\n(C) yellow\n(D) green\n(E) blue\n(F) brown\n(G) magenta\n(H) fuchsia\n(I) mauve\n(J) teal\n(K) turquoise\n(L) burgundy\n(M) silver\n(N) gold\n(O) black\n(P) grey\n(Q) purple\n(R) pink",
      "prediction": "(m)",
      "true_answer": "(m)"
    }
  ]
}
{
  "task": "ruin_names",
  "ZSL_1_accuracy": 53.6,
  "eval_time": 3.1242,
  "examples": [
    {
      "question": "Which of the following is a humorous edit of this artist or movie name: 'block 57'?\nOptions:\n(A) blonck 57\n(B) blockv57\n(C) bzlock 57\n(D) block 47",
      "prediction": "(a)",
      "true_answer": "(d)"
    },
    {
      "question": "Which of the following is a humorous edit of this artist or movie name: 'belle and sebastian'?\nOptions:\n(A) belle and sebasrtian\n(B) belly and sebastian\n(C) belle and sebajtian\n(D) ibelle and sebastian",
      "prediction": "(b)",
      "true_answer": "(b)"
    },
    {
      "question": "Which of the following is a humorous edit of this artist or movie name: 'state champs'?\nOptions:\n(A) state champ\n(B) statek champs\n(C) state cqhamps\n(D) state chumps",
      "prediction": "(d)",
      "true_answer": "(d)"
    },
    {
      "question": "Which of the following is a humorous edit of this artist or movie name: 'lawrence of arabia'?\nOptions:\n(A) lawrence of arabida\n(B) ljwrence of arabia\n(C) lawrence of arabica\n(D) lawrenqe of arabia",
      "prediction": "(a)",
      "true_answer": "(c)"
    },
    {
      "question": "Which of the following is a humorous edit of this artist or movie name: 'johnny got his gun'?\nOptions:\n(A) johnny got hiw gun\n(B) johnno got his gun\n(C) johnny got his run\n(D) johgnny got his gun",
      "prediction": "(a)",
      "true_answer": "(c)"
    }
  ]
}
{
  "task": "salient_translation_error_detection",
  "ZSL_1_accuracy": 42.0,
  "eval_time": 9.373,
  "examples": [
    {
      "question": "The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Mittelalterliche Bronzef\u00fcnten des niederdeutschen Kulturraums sind Taufbecken aus Bronze, die, beginnend mit dem 13.\nTranslation: Medieval bronze feet of the Lower German cultural area are baptismal fonts made of gold, which, beginning with the 13th.\nThe translation contains an error pertaining to\nOptions:\n(A) Modifiers or Adjectives\n(B) Numerical Values\n(C) Negation or Antonyms\n(D) Named Entities\n(E) Dropped Content\n(F) Facts",
      "prediction": "(b)",
      "true_answer": "(f)"
    },
    {
      "question": "The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Die Talbr\u00fccke Steinatal ist eine 445 m lange Br\u00fccke der Autobahn 71.\nTranslation: The Steinatal Valley Bridge is a 45 m long bridge on highway 71.\nThe translation contains an error pertaining to\nOptions:\n(A) Modifiers or Adjectives\n(B) Numerical Values\n(C) Negation or Antonyms\n(D) Named Entities\n(E) Dropped Content\n(F) Facts",
      "prediction": "(b)",
      "true_answer": "(b)"
    },
    {
      "question": "The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Karte mit allen Koordinaten: OSM | WikiMap\nTranslation: Map with no coordinates: OSM | WikiMap\nThe translation contains an error pertaining to\nOptions:\n(A) Modifiers or Adjectives\n(B) Numerical Values\n(C) Negation or Antonyms\n(D) Named Entities\n(E) Dropped Content\n(F) Facts",
      "prediction": "(e)",
      "true_answer": "(c)"
    },
    {
      "question": "The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Als Supercomputer werden f\u00fcr ihre Zeit besonders schnelle Computer bezeichnet.\nTranslation: Supercomputers are called particularly small computers for their time.\nThe translation contains an error pertaining to\nOptions:\n(A) Modifiers or Adjectives\n(B) Numerical Values\n(C) Negation or Antonyms\n(D) Named Entities\n(E) Dropped Content\n(F) Facts",
      "prediction": "(a)",
      "true_answer": "(a)"
    },
    {
      "question": "The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Die Centralv\u00e6rkstedet K\u00f8benhavn in der d\u00e4nischen Hauptstadt Kopenhagen wurde 1847 gegr\u00fcndet.\nTranslation: The Centralv\u00e9rkstedet Kobenhavn in Copenhagen, Denmark, was destroyed in 1847.\nThe translation contains an error pertaining to\nOptions:\n(A) Modifiers or Adjectives\n(B) Numerical Values\n(C) Negation or Antonyms\n(D) Named Entities\n(E) Dropped Content\n(F) Facts",
      "prediction": "(e)",
      "true_answer": "(c)"
    }
  ]
}
{
  "task": "snarks",
  "ZSL_1_accuracy": 60.6742,
  "eval_time": 1.9264,
  "examples": [
    {
      "question": "Which statement is sarcastic?\nOptions:\n(A) how dare you use violence!!!\n(B) how dare you use logic!!!",
      "prediction": "(b)",
      "true_answer": "(b)"
    },
    {
      "question": "Which statement is sarcastic?\nOptions:\n(A) Hey genocide is clearly the most rational decision\n(B) Hey compromise is clearly the most rational decision",
      "prediction": "(a)",
      "true_answer": "(a)"
    },
    {
      "question": "Which statement is sarcastic?\nOptions:\n(A) The best way to defeat a stronger/larger and better regimented force is by fighting on their terms\n(B) The best way to defeat a stronger/larger and better regimented force is by fighting on your terms",
      "prediction": "(a)",
      "true_answer": "(a)"
    },
    {
      "question": "Which statement is sarcastic?\nOptions:\n(A) Since corporate restriction equals free speech this is actually a big win for democracy\n(B) Since corporate money equals free speech this is actually a big win for democracy",
      "prediction": "(a)",
      "true_answer": "(b)"
    },
    {
      "question": "Which statement is sarcastic?\nOptions:\n(A) Everyone knows salt is the highlight of gourmet sophistication\n(B) Everyone knows caviar is the highlight of gourmet sophistication",
      "prediction": "(a)",
      "true_answer": "(a)"
    }
  ]
}
{
  "task": "sports_understanding",
  "ZSL_1_accuracy": 76.0,
  "eval_time": 1.6564,
  "examples": [
    {
      "question": "Is the following sentence plausible? \"Santi Cazorla called for the screen.\"",
      "prediction": "yes",
      "true_answer": "no"
    },
    {
      "question": "Is the following sentence plausible? \"Ben Simmons was called for the goal tend.\"",
      "prediction": "no",
      "true_answer": "yes"
    },
    {
      "question": "Is the following sentence plausible? \"Stefon Diggs hit the slant pass.\"",
      "prediction": "yes",
      "true_answer": "yes"
    },
    {
      "question": "Is the following sentence plausible? \"Joe Burrow gained five yards.\"",
      "prediction": "yes",
      "true_answer": "yes"
    },
    {
      "question": "Is the following sentence plausible? \"Malcolm Brogdon eurostepped to the basket in the NBA Championship.\"",
      "prediction": "yes",
      "true_answer": "yes"
    }
  ]
}
{
  "task": "temporal_sequences",
  "ZSL_1_accuracy": 93.2,
  "eval_time": 6.6654,
  "examples": [
    {
      "question": "Today, Michael went to the physics classroom. Between what times could they have gone?\nWe know that:\nMichael woke up at 5am.\nSamantha saw Michael buying lunch at the deli from 5am to 8am.\nJennifer saw Michael stretching at a yoga studio from 8am to 9am.\nAndrew saw Michael taking photos near the Leaning Tower of Pisa from 1pm to 10pm.\nThe physics classroom was closed after 10pm.\nBetween what times could Michael have gone to the physics classroom?\nOptions:\n(A) 9am to 1pm\n(B) 1pm to 10pm\n(C) 5am to 8am\n(D) 8am to 9am",
      "prediction": "(a)",
      "true_answer": "(a)"
    },
    {
      "question": "Today, Ashley went to the basketball court. Between what times could they have gone?\nWe know that:\nAshley woke up at 9am.\nJohn saw Ashley taking photos near the Eiffel Tower from 9am to 10am.\nLinda saw Ashley walking in the garden from 10am to 1pm.\nAshley saw Ashley attending class at the school from 1pm to 2pm.\nThe basketball court was closed after 7pm.\nBetween what times could Ashley have gone to the basketball court?\nOptions:\n(A) 2pm to 7pm\n(B) 9am to 10am\n(C) 1pm to 2pm\n(D) 10am to 1pm",
      "prediction": "(a)",
      "true_answer": "(a)"
    },
    {
      "question": "Today, Sarah went to the gas station. Between what times could they have gone?\nWe know that:\nSarah woke up at 7am.\nKimberly saw Sarah sitting on a rooftop from 7am to 9am.\nMark saw Sarah buying a bike at the bike shop from 11am to 2pm.\nJohn saw Sarah buying cookies at a bakery from 2pm to 3pm.\nWilliam saw Sarah fixing their computer at the electronic store from 3pm to 4pm.\nThe gas station was closed after 4pm.\nBetween what times could Sarah have gone to the gas station?\nOptions:\n(A) 11am to 2pm\n(B) 2pm to 3pm\n(C) 9am to 11am\n(D) 3pm to 4pm",
      "prediction": "(c)",
      "true_answer": "(c)"
    },
    {
      "question": "Today, David went to the construction site. Between what times could they have gone?\nWe know that:\nDavid woke up at 7am.\nMichael saw David reading at the library from 7am to 9am.\nWilliam saw David waiting at the airport from 6pm to 8pm.\nLisa saw David waiting at the train station from 8pm to 10pm.\nThe construction site was closed after 10pm.\nBetween what times could David have gone to the construction site?\nOptions:\n(A) 7am to 9am\n(B) 9am to 6pm\n(C) 8pm to 10pm\n(D) 6pm to 8pm",
      "prediction": "(b)",
      "true_answer": "(b)"
    },
    {
      "question": "Today, David went to the physics classroom. Between what times could they have gone?\nWe know that:\nDavid woke up at 9am.\nSean saw David fixing their computer at the electronic store from 9am to 11am.\nThomas saw David walking towards the Statue of Liberty from 11am to 12pm.\nJames saw David taking photos near the Leaning Tower of Pisa from 12pm to 8pm.\nThe physics classroom was closed after 10pm.\nBetween what times could David have gone to the physics classroom?\nOptions:\n(A) 11am to 12pm\n(B) 12pm to 8pm\n(C) 9am to 11am\n(D) 8pm to 10pm",
      "prediction": "(d)",
      "true_answer": "(d)"
    }
  ]
}
{
  "task": "tracking_shuffled_objects_five_objects",
  "ZSL_1_accuracy": 18.8,
  "eval_time": 6.7752,
  "examples": [
    {
      "question": "Alice, Bob, Claire, Dave, and Eve are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Helga, Bob is dancing with Rodrigo, Claire is dancing with Melissa, Dave is dancing with Patrick, and Eve is dancing with Jamie.\nThroughout the song, the dancers often trade partners. First, Eve and Bob switch partners. Then, Alice and Bob switch partners. Then, Dave and Eve switch partners. Then, Eve and Claire switch partners. Finally, Alice and Claire switch partners. At the end of the dance, Eve is dancing with\nOptions:\n(A) Helga\n(B) Rodrigo\n(C) Melissa\n(D) Patrick\n(E) Jamie",
      "prediction": "(d)",
      "true_answer": "(c)"
    },
    {
      "question": "Alice, Bob, Claire, Dave, and Eve are friends and avid readers who occasionally trade books. At the start of the semester, they each buy one new book: Alice gets The Great Gatsby, Bob gets Frankenstein, Claire gets The Pearl, Dave gets Catch-22, and Eve gets Ulysses.\nAs the semester proceeds, they start trading around the new books. First, Claire and Dave swap books. Then, Bob and Alice swap books. Then, Dave and Alice swap books. Then, Bob and Claire swap books. Finally, Dave and Eve swap books. At the end of the semester, Eve has\nOptions:\n(A) The Great Gatsby\n(B) Frankenstein\n(C) The Pearl\n(D) Catch-22\n(E) Ulysses",
      "prediction": "(d)",
      "true_answer": "(b)"
    },
    {
      "question": "Alice, Bob, Claire, Dave, and Eve are on the same team in a soccer match. At the start of the match, they are each assigned to a position: Alice is playing right midfielder, Bob is playing fullback, Claire is playing striker, Dave is playing goalkeeper, and Eve is playing benchwarmer.\nAs the game progresses, pairs of players occasionally swap positions. First, Eve and Bob trade positions. Then, Bob and Claire trade positions. Then, Dave and Alice trade positions. Then, Eve and Claire trade positions. Finally, Alice and Bob trade positions. At the end of the match, Alice is playing\nOptions:\n(A) right midfielder\n(B) fullback\n(C) striker\n(D) goalkeeper\n(E) benchwarmer",
      "prediction": "(b)",
      "true_answer": "(c)"
    },
    {
      "question": "Alice, Bob, Claire, Dave, and Eve are on the same team in a soccer match. At the start of the match, they are each assigned to a position: Alice is playing fullback, Bob is playing right winger, Claire is playing center midfielder, Dave is playing striker, and Eve is playing benchwarmer.\nAs the game progresses, pairs of players occasionally swap positions. First, Alice and Bob trade positions. Then, Dave and Bob trade positions. Then, Alice and Claire trade positions. Then, Bob and Claire trade positions. Finally, Alice and Eve trade positions. At the end of the match, Dave is playing\nOptions:\n(A) fullback\n(B) right winger\n(C) center midfielder\n(D) striker\n(E) benchwarmer",
      "prediction": "(b)",
      "true_answer": "(a)"
    },
    {
      "question": "Alice, Bob, Claire, Dave, and Eve are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Helga, Bob is dancing with Karl, Claire is dancing with Izzi, Dave is dancing with Sam, and Eve is dancing with Melissa.\nThroughout the song, the dancers often trade partners. First, Bob and Alice switch partners. Then, Bob and Claire switch partners. Then, Dave and Claire switch partners. Then, Eve and Dave switch partners. Finally, Bob and Alice switch partners. At the end of the dance, Eve is dancing with\nOptions:\n(A) Helga\n(B) Karl\n(C) Izzi\n(D) Sam\n(E) Melissa",
      "prediction": "(a)",
      "true_answer": "(a)"
    }
  ]
}
{
  "task": "tracking_shuffled_objects_seven_objects",
  "ZSL_1_accuracy": 16.8,
  "eval_time": 8.3648,
  "examples": [
    {
      "question": "Alice, Bob, Claire, Dave, Eve, Fred, and Gertrude are playing a game. At the start of the game, they are each holding a ball: Alice has a blue ball, Bob has a green ball, Claire has a brown ball, Dave has a pink ball, Eve has a orange ball, Fred has a black ball, and Gertrude has a white ball.\nAs the game progresses, pairs of players trade balls. First, Gertrude and Alice swap balls. Then, Claire and Eve swap balls. Then, Bob and Claire swap balls. Then, Claire and Eve swap balls. Then, Eve and Fred swap balls. Then, Bob and Fred swap balls. Finally, Dave and Eve swap balls. At the end of the game, Alice has the\nOptions:\n(A) blue ball\n(B) green ball\n(C) brown ball\n(D) pink ball\n(E) orange ball\n(F) black ball\n(G) white ball",
      "prediction": "(g)",
      "true_answer": "(g)"
    },
    {
      "question": "Alice, Bob, Claire, Dave, Eve, Fred, and Gertrude are playing a game. At the start of the game, they are each holding a ball: Alice has a green ball, Bob has a white ball, Claire has a yellow ball, Dave has a pink ball, Eve has a orange ball, Fred has a black ball, and Gertrude has a brown ball.\nAs the game progresses, pairs of players trade balls. First, Bob and Gertrude swap balls. Then, Fred and Claire swap balls. Then, Dave and Gertrude swap balls. Then, Bob and Gertrude swap balls. Then, Alice and Claire swap balls. Then, Gertrude and Claire swap balls. Finally, Eve and Claire swap balls. At the end of the game, Fred has the\nOptions:\n(A) green ball\n(B) white ball\n(C) yellow ball\n(D) pink ball\n(E) orange ball\n(F) black ball\n(G) brown ball",
      "prediction": "(c)",
      "true_answer": "(c)"
    },
    {
      "question": "Alice, Bob, Claire, Dave, Eve, Fred, and Gertrude are playing a game. At the start of the game, they are each holding a ball: Alice has a white ball, Bob has a black ball, Claire has a pink ball, Dave has a orange ball, Eve has a purple ball, Fred has a green ball, and Gertrude has a brown ball.\nAs the game progresses, pairs of players trade balls. First, Eve and Claire swap balls. Then, Bob and Eve swap balls. Then, Claire and Alice swap balls. Then, Bob and Alice swap balls. Then, Dave and Eve swap balls. Then, Fred and Gertrude swap balls. Finally, Gertrude and Claire swap balls. At the end of the game, Fred has the\nOptions:\n(A) white ball\n(B) black ball\n(C) pink ball\n(D) orange ball\n(E) purple ball\n(F) green ball\n(G) brown ball",
      "prediction": "(f)",
      "true_answer": "(g)"
    },
    {
      "question": "Alice, Bob, Claire, Dave, Eve, Fred, and Gertrude are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Jamie, Bob is dancing with Ophelia, Claire is dancing with Lola, Dave is dancing with Patrick, Eve is dancing with Rodrigo, Fred is dancing with Melissa, and Gertrude is dancing with Karl.\nThroughout the song, the dancers often trade partners. First, Eve and Dave switch partners. Then, Fred and Eve switch partners. Then, Alice and Gertrude switch partners. Then, Dave and Bob switch partners. Then, Alice and Claire switch partners. Then, Alice and Gertrude switch partners. Finally, Bob and Claire switch partners. At the end of the dance, Fred is dancing with\nOptions:\n(A) Jamie\n(B) Ophelia\n(C) Lola\n(D) Patrick\n(E) Rodrigo\n(F) Melissa\n(G) Karl",
      "prediction": "(f)",
      "true_answer": "(d)"
    },
    {
      "question": "Alice, Bob, Claire, Dave, Eve, Fred, and Gertrude are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Izzi, Bob is dancing with Helga, Claire is dancing with Melissa, Dave is dancing with Sam, Eve is dancing with Karl, Fred is dancing with Ophelia, and Gertrude is dancing with Lola.\nThroughout the song, the dancers often trade partners. First, Fred and Eve switch partners. Then, Bob and Dave switch partners. Then, Eve and Alice switch partners. Then, Bob and Claire switch partners. Then, Eve and Alice switch partners. Then, Gertrude and Fred switch partners. Finally, Bob and Dave switch partners. At the end of the dance, Fred is dancing with\nOptions:\n(A) Izzi\n(B) Helga\n(C) Melissa\n(D) Sam\n(E) Karl\n(F) Ophelia\n(G) Lola",
      "prediction": "(f)",
      "true_answer": "(g)"
    }
  ]
}
{
  "task": "tracking_shuffled_objects_three_objects",
  "ZSL_1_accuracy": 30.0,
  "eval_time": 5.4106,
  "examples": [
    {
      "question": "Alice, Bob, and Claire are friends and avid readers who occasionally trade books. At the start of the semester, they each buy one new book: Alice gets Catch-22, Bob gets Moby Dick, and Claire gets The Great Gatsby.\nAs the semester proceeds, they start trading around the new books. First, Claire and Bob swap books. Then, Claire and Alice swap books. Finally, Bob and Claire swap books. At the end of the semester, Claire has\nOptions:\n(A) Catch-22\n(B) Moby Dick\n(C) The Great Gatsby",
      "prediction": "(c)",
      "true_answer": "(c)"
    },
    {
      "question": "Alice, Bob, and Claire are holding a white elephant gift exchange. At the start of the event, they are each holding a present of a different color: Alice has a green present, Bob has a blue present, and Claire has a brown present.\nAs the event progresses, pairs of people swap gifts. First, Claire and Alice swap their gifts. Then, Claire and Bob swap their gifts. Finally, Alice and Claire swap their gifts. At the end of the event, Claire has the\nOptions:\n(A) green present\n(B) blue present\n(C) brown present",
      "prediction": "(b)",
      "true_answer": "(c)"
    },
    {
      "question": "Alice, Bob, and Claire are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Sam, Bob is dancing with Jamie, and Claire is dancing with Melissa.\nThroughout the song, the dancers often trade partners. First, Alice and Claire switch partners. Then, Claire and Bob switch partners. Finally, Alice and Bob switch partners. At the end of the dance, Alice is dancing with\nOptions:\n(A) Sam\n(B) Jamie\n(C) Melissa",
      "prediction": "(b)",
      "true_answer": "(a)"
    },
    {
      "question": "Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a pink ball, Bob has a red ball, and Claire has a green ball.\nAs the game progresses, pairs of players trade balls. First, Claire and Bob swap balls. Then, Alice and Claire swap balls. Finally, Claire and Bob swap balls. At the end of the game, Bob has the\nOptions:\n(A) pink ball\n(B) red ball\n(C) green ball",
      "prediction": "(b)",
      "true_answer": "(a)"
    },
    {
      "question": "Alice, Bob, and Claire are on the same team in a soccer match. At the start of the match, they are each assigned to a position: Alice is playing left midfielder, Bob is playing goalkeeper, and Claire is playing right winger.\nAs the game progresses, pairs of players occasionally swap positions. First, Claire and Bob trade positions. Then, Claire and Alice trade positions. Finally, Alice and Bob trade positions. At the end of the match, Bob is playing\nOptions:\n(A) left midfielder\n(B) goalkeeper\n(C) right winger",
      "prediction": "(c)",
      "true_answer": "(b)"
    }
  ]
}
{
  "task": "web_of_lies",
  "ZSL_1_accuracy": 0.0,
  "eval_time": 2.4414,
  "examples": [
    {
      "question": "Question: Millicent lies. Alexis says Millicent lies. Inga says Alexis tells the truth. Christie says Inga lies. Gwenn says Christie lies. Does Gwenn tell the truth?",
      "prediction": "a",
      "true_answer": "yes"
    },
    {
      "question": "Question: Millicent tells the truth. Alexis says Millicent lies. Jaymie says Alexis lies. Willian says Jaymie tells the truth. Leda says Willian lies. Does Leda tell the truth?",
      "prediction": "##",
      "true_answer": "no"
    },
    {
      "question": "Question: Leda lies. Fidel says Leda lies. Crista says Fidel tells the truth. Ka says Crista lies. Jim says Ka tells the truth. Does Jim tell the truth?",
      "prediction": "##",
      "true_answer": "no"
    },
    {
      "question": "Question: Bernita lies. Jerry says Bernita tells the truth. Delbert says Jerry lies. Jamey says Delbert tells the truth. Shenna says Jamey lies. Does Shenna tell the truth?",
      "prediction": "a",
      "true_answer": "no"
    },
    {
      "question": "Question: Alejandro tells the truth. Shenna says Alejandro tells the truth. Christie says Shenna tells the truth. Shalonda says Christie lies. Jerry says Shalonda lies. Does Jerry tell the truth?",
      "prediction": "(",
      "true_answer": "yes"
    }
  ]
}
{
  "task": "word_sorting",
  "ZSL_1_accuracy": 50.0,
  "eval_time": 4.2214,
  "examples": [
    {
      "question": "Sort the following words alphabetically: List: wintertime mobster ugh brown fleabane longish ipsilateral cortex hair diminutive trapezoidal guy richfield highfalutin borough foot delphinium cosec abstract",
      "prediction": "abstract borough brown cortex delphinium diminutive fleabane foot guy highfalutin ipsilateral longish mobster richfield trapezoidal ugh wintertime",
      "true_answer": "abstract borough brown cortex cosec delphinium diminutive fleabane foot guy hair highfalutin ipsilateral longish mobster richfield trapezoidal ugh wintertime"
    },
    {
      "question": "Sort the following words alphabetically: List: deform budd",
      "prediction": "budd deform",
      "true_answer": "budd deform"
    },
    {
      "question": "Sort the following words alphabetically: List: windowsill appoint biharmonic moustache baneberry wiry dyne pirate",
      "prediction": "appoint baneberry biharmonic dyne moustache pirate windowsill wiry",
      "true_answer": "appoint baneberry biharmonic dyne moustache pirate windowsill wiry"
    },
    {
      "question": "Sort the following words alphabetically: List: sibyl calligraph goat inverness form threadbare",
      "prediction": "calligraph form goat inverness sibyl threadbare",
      "true_answer": "calligraph form goat inverness sibyl threadbare"
    },
    {
      "question": "Sort the following words alphabetically: List: regret uptake satisfy behold fumarole motif forth alcohol escutcheon pease hackberry walkie",
      "prediction": "behold alcohol escutcheon fumarole forth hackberry motif pease regret satisfy uptake walkie",
      "true_answer": "alcohol behold escutcheon forth fumarole hackberry motif pease regret satisfy uptake walkie"
    }
  ]
}

Saved baseline results to logs/current/20250121_ZSL_1_gpu0.json.
[rank0]:[W121 21:17:30.601590456 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
